{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f99360-2a08-41d5-9ded-b7912486fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from compressai.datasets import ImageFolder\n",
    "from compressai.zoo import image_models\n",
    "from compressai.optimizers import net_aux_optimizer\n",
    "from compressai.losses import RateDistortionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33133bc-1595-428c-b264-e7ab306cbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='bmshj2018-factorized'\n",
    "dataset='/home/server/datasets/example'\n",
    "epochs=100\n",
    "learning_rate=0.0001\n",
    "num_workers=1\n",
    "lmbda=0.01\n",
    "batch_size=16\n",
    "test_batch_size=64\n",
    "aux_learning_rate=0.001\n",
    "patch_size=(256, 256)\n",
    "cuda=True\n",
    "save=True\n",
    "seed=0\n",
    "clip_max_norm=1.0\n",
    "checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948163ad-5683-4762-9c25-a1efeb30dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(net):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "    conf = {\n",
    "        \"net\": {\"type\": \"Adam\", \"lr\": learning_rate},\n",
    "        \"aux\": {\"type\": \"Adam\", \"lr\": aux_learning_rate},\n",
    "    }\n",
    "    optimizer = net_aux_optimizer(net, conf)\n",
    "    return optimizer[\"net\"], optimizer[\"aux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7dec53-a0d3-4614-afac-8c5ef9efa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "\n",
    "        out_net = model(d)\n",
    "\n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward()\n",
    "        if clip_max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\tMSE loss: {out_criterion[\"mse_loss\"].item():.3f} |'\n",
    "                f'\\tBpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'\n",
    "                f\"\\tAux loss: {aux_loss.item():.2f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75f414e-0002-49c2-b370-247a4b646c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(epoch, test_dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    mse_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "            out_net = model(d)\n",
    "            out_criterion = criterion(out_net, d)\n",
    "\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
    "\n",
    "    print(\n",
    "        f\"Test epoch {epoch}: Average losses:\"\n",
    "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "        f\"\\tMSE loss: {mse_loss.avg:.3f} |\"\n",
    "        f\"\\tBpp loss: {bpp_loss.avg:.2f} |\"\n",
    "        f\"\\tAux loss: {aux_loss.avg:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    return loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5315e0-20e2-46e1-bf6f-7dc6170ca724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5972b3b-5374-4d0a-8388-1774f8b06a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"checkpoint_best_loss.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c19ca3-9318-4837-b59c-754911dc26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder(dataset, split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder(dataset, split=\"test\", transform=test_transforms)\n",
    "\n",
    "device = \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "net = image_models[model](quality=3)\n",
    "net = net.to(device)\n",
    "optimizer, aux_optimizer = configure_optimizers(net)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "criterion = RateDistortionLoss(lmbda=lmbda)\n",
    "last_epoch = 0\n",
    "\n",
    "if checkpoint:  # load from previous checkpoint\n",
    "    print(\"Loading\", checkpoint)\n",
    "    checkpoint = torch.load(checkpoint, map_location=device)\n",
    "    last_epoch = checkpoint[\"epoch\"] + 1\n",
    "    net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "\n",
    "best_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d67201b-6c5d-4ed4-9fe9-4dd3aa8ad175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Train epoch 0: [0/318 (0%)]\tLoss: 22.133 |\tMSE loss: 0.028 |\tBpp loss: 4.01 |\tAux loss: 7899.10\n",
      "Train epoch 0: [160/318 (50%)]\tLoss: 31.995 |\tMSE loss: 0.043 |\tBpp loss: 4.01 |\tAux loss: 7895.50\n",
      "Test epoch 0: Average losses:\tLoss: 23.009 |\tMSE loss: 0.029 |\tBpp loss: 4.00 |\tAux loss: 7892.14\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 1: [0/318 (0%)]\tLoss: 21.483 |\tMSE loss: 0.027 |\tBpp loss: 4.00 |\tAux loss: 7891.82\n",
      "Train epoch 1: [160/318 (50%)]\tLoss: 16.069 |\tMSE loss: 0.019 |\tBpp loss: 3.99 |\tAux loss: 7887.92\n",
      "Test epoch 1: Average losses:\tLoss: 25.086 |\tMSE loss: 0.032 |\tBpp loss: 3.98 |\tAux loss: 7884.15\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 2: [0/318 (0%)]\tLoss: 27.356 |\tMSE loss: 0.036 |\tBpp loss: 3.98 |\tAux loss: 7883.79\n",
      "Train epoch 2: [160/318 (50%)]\tLoss: 17.128 |\tMSE loss: 0.020 |\tBpp loss: 3.97 |\tAux loss: 7879.62\n",
      "Test epoch 2: Average losses:\tLoss: 21.784 |\tMSE loss: 0.027 |\tBpp loss: 3.96 |\tAux loss: 7875.67\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 3: [0/318 (0%)]\tLoss: 15.361 |\tMSE loss: 0.018 |\tBpp loss: 3.96 |\tAux loss: 7875.29\n",
      "Train epoch 3: [160/318 (50%)]\tLoss: 27.206 |\tMSE loss: 0.036 |\tBpp loss: 3.95 |\tAux loss: 7870.70\n",
      "Test epoch 3: Average losses:\tLoss: 19.772 |\tMSE loss: 0.024 |\tBpp loss: 3.94 |\tAux loss: 7866.27\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 4: [0/318 (0%)]\tLoss: 20.598 |\tMSE loss: 0.026 |\tBpp loss: 3.94 |\tAux loss: 7865.81\n",
      "Train epoch 4: [160/318 (50%)]\tLoss: 19.075 |\tMSE loss: 0.023 |\tBpp loss: 3.93 |\tAux loss: 7860.82\n",
      "Test epoch 4: Average losses:\tLoss: 20.672 |\tMSE loss: 0.026 |\tBpp loss: 3.92 |\tAux loss: 7856.60\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 5: [0/318 (0%)]\tLoss: 14.836 |\tMSE loss: 0.017 |\tBpp loss: 3.92 |\tAux loss: 7856.22\n",
      "Train epoch 5: [160/318 (50%)]\tLoss: 14.803 |\tMSE loss: 0.017 |\tBpp loss: 3.91 |\tAux loss: 7851.59\n",
      "Test epoch 5: Average losses:\tLoss: 14.150 |\tMSE loss: 0.016 |\tBpp loss: 3.90 |\tAux loss: 7847.30\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 6: [0/318 (0%)]\tLoss: 12.939 |\tMSE loss: 0.014 |\tBpp loss: 3.90 |\tAux loss: 7846.88\n",
      "Train epoch 6: [160/318 (50%)]\tLoss: 12.008 |\tMSE loss: 0.012 |\tBpp loss: 3.89 |\tAux loss: 7841.52\n",
      "Test epoch 6: Average losses:\tLoss: 14.170 |\tMSE loss: 0.016 |\tBpp loss: 3.87 |\tAux loss: 7835.21\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 7: [0/318 (0%)]\tLoss: 14.660 |\tMSE loss: 0.017 |\tBpp loss: 3.87 |\tAux loss: 7834.62\n",
      "Train epoch 7: [160/318 (50%)]\tLoss: 11.094 |\tMSE loss: 0.011 |\tBpp loss: 3.86 |\tAux loss: 7828.21\n",
      "Test epoch 7: Average losses:\tLoss: 13.911 |\tMSE loss: 0.015 |\tBpp loss: 3.84 |\tAux loss: 7822.34\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 8: [0/318 (0%)]\tLoss: 18.620 |\tMSE loss: 0.023 |\tBpp loss: 3.84 |\tAux loss: 7821.88\n",
      "Train epoch 8: [160/318 (50%)]\tLoss: 9.325 |\tMSE loss: 0.008 |\tBpp loss: 3.83 |\tAux loss: 7817.16\n",
      "Test epoch 8: Average losses:\tLoss: 11.339 |\tMSE loss: 0.012 |\tBpp loss: 3.82 |\tAux loss: 7812.85\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 9: [0/318 (0%)]\tLoss: 9.739 |\tMSE loss: 0.009 |\tBpp loss: 3.82 |\tAux loss: 7812.33\n",
      "Train epoch 9: [160/318 (50%)]\tLoss: 10.590 |\tMSE loss: 0.010 |\tBpp loss: 3.81 |\tAux loss: 7806.55\n",
      "Test epoch 9: Average losses:\tLoss: 11.954 |\tMSE loss: 0.013 |\tBpp loss: 3.80 |\tAux loss: 7800.33\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 10: [0/318 (0%)]\tLoss: 12.260 |\tMSE loss: 0.013 |\tBpp loss: 3.80 |\tAux loss: 7799.60\n",
      "Train epoch 10: [160/318 (50%)]\tLoss: 9.425 |\tMSE loss: 0.009 |\tBpp loss: 3.78 |\tAux loss: 7791.42\n",
      "Test epoch 10: Average losses:\tLoss: 10.991 |\tMSE loss: 0.011 |\tBpp loss: 3.76 |\tAux loss: 7784.46\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 11: [0/318 (0%)]\tLoss: 9.765 |\tMSE loss: 0.009 |\tBpp loss: 3.77 |\tAux loss: 7783.87\n",
      "Train epoch 11: [160/318 (50%)]\tLoss: 12.464 |\tMSE loss: 0.013 |\tBpp loss: 3.75 |\tAux loss: 7777.88\n",
      "Test epoch 11: Average losses:\tLoss: 11.936 |\tMSE loss: 0.013 |\tBpp loss: 3.74 |\tAux loss: 7772.88\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 12: [0/318 (0%)]\tLoss: 9.573 |\tMSE loss: 0.009 |\tBpp loss: 3.74 |\tAux loss: 7772.48\n",
      "Train epoch 12: [160/318 (50%)]\tLoss: 12.908 |\tMSE loss: 0.014 |\tBpp loss: 3.73 |\tAux loss: 7768.14\n",
      "Test epoch 12: Average losses:\tLoss: 10.204 |\tMSE loss: 0.010 |\tBpp loss: 3.72 |\tAux loss: 7764.08\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 13: [0/318 (0%)]\tLoss: 8.462 |\tMSE loss: 0.007 |\tBpp loss: 3.73 |\tAux loss: 7763.70\n",
      "Train epoch 13: [160/318 (50%)]\tLoss: 10.004 |\tMSE loss: 0.010 |\tBpp loss: 3.72 |\tAux loss: 7759.13\n",
      "Test epoch 13: Average losses:\tLoss: 10.120 |\tMSE loss: 0.010 |\tBpp loss: 3.71 |\tAux loss: 7754.78\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 14: [0/318 (0%)]\tLoss: 11.272 |\tMSE loss: 0.012 |\tBpp loss: 3.71 |\tAux loss: 7754.34\n",
      "Train epoch 14: [160/318 (50%)]\tLoss: 10.333 |\tMSE loss: 0.010 |\tBpp loss: 3.70 |\tAux loss: 7749.29\n",
      "Test epoch 14: Average losses:\tLoss: 10.039 |\tMSE loss: 0.010 |\tBpp loss: 3.69 |\tAux loss: 7744.73\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 15: [0/318 (0%)]\tLoss: 9.836 |\tMSE loss: 0.009 |\tBpp loss: 3.69 |\tAux loss: 7744.28\n",
      "Train epoch 15: [160/318 (50%)]\tLoss: 8.087 |\tMSE loss: 0.007 |\tBpp loss: 3.68 |\tAux loss: 7739.13\n",
      "Test epoch 15: Average losses:\tLoss: 10.090 |\tMSE loss: 0.010 |\tBpp loss: 3.67 |\tAux loss: 7734.47\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 16: [0/318 (0%)]\tLoss: 10.061 |\tMSE loss: 0.010 |\tBpp loss: 3.67 |\tAux loss: 7734.03\n",
      "Train epoch 16: [160/318 (50%)]\tLoss: 10.805 |\tMSE loss: 0.011 |\tBpp loss: 3.66 |\tAux loss: 7729.23\n",
      "Test epoch 16: Average losses:\tLoss: 10.175 |\tMSE loss: 0.010 |\tBpp loss: 3.65 |\tAux loss: 7724.84\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 17: [0/318 (0%)]\tLoss: 7.406 |\tMSE loss: 0.006 |\tBpp loss: 3.65 |\tAux loss: 7724.46\n",
      "Train epoch 17: [160/318 (50%)]\tLoss: 11.883 |\tMSE loss: 0.013 |\tBpp loss: 3.64 |\tAux loss: 7719.73\n",
      "Test epoch 17: Average losses:\tLoss: 10.358 |\tMSE loss: 0.010 |\tBpp loss: 3.63 |\tAux loss: 7715.50\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 18: [0/318 (0%)]\tLoss: 12.022 |\tMSE loss: 0.013 |\tBpp loss: 3.63 |\tAux loss: 7715.11\n",
      "Train epoch 18: [160/318 (50%)]\tLoss: 11.924 |\tMSE loss: 0.013 |\tBpp loss: 3.63 |\tAux loss: 7710.33\n",
      "Test epoch 18: Average losses:\tLoss: 10.343 |\tMSE loss: 0.010 |\tBpp loss: 3.62 |\tAux loss: 7705.67\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 19: [0/318 (0%)]\tLoss: 8.808 |\tMSE loss: 0.008 |\tBpp loss: 3.62 |\tAux loss: 7705.23\n",
      "Train epoch 19: [160/318 (50%)]\tLoss: 9.662 |\tMSE loss: 0.009 |\tBpp loss: 3.61 |\tAux loss: 7700.21\n",
      "Test epoch 19: Average losses:\tLoss: 10.242 |\tMSE loss: 0.010 |\tBpp loss: 3.60 |\tAux loss: 7695.83\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 20: [0/318 (0%)]\tLoss: 6.331 |\tMSE loss: 0.004 |\tBpp loss: 3.60 |\tAux loss: 7695.43\n",
      "Train epoch 20: [160/318 (50%)]\tLoss: 11.686 |\tMSE loss: 0.012 |\tBpp loss: 3.59 |\tAux loss: 7690.54\n",
      "Test epoch 20: Average losses:\tLoss: 10.323 |\tMSE loss: 0.010 |\tBpp loss: 3.58 |\tAux loss: 7685.66\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 21: [0/318 (0%)]\tLoss: 10.385 |\tMSE loss: 0.010 |\tBpp loss: 3.58 |\tAux loss: 7685.22\n",
      "Train epoch 21: [160/318 (50%)]\tLoss: 7.318 |\tMSE loss: 0.006 |\tBpp loss: 3.57 |\tAux loss: 7680.18\n",
      "Test epoch 21: Average losses:\tLoss: 10.043 |\tMSE loss: 0.010 |\tBpp loss: 3.57 |\tAux loss: 7675.41\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 22: [0/318 (0%)]\tLoss: 14.976 |\tMSE loss: 0.018 |\tBpp loss: 3.57 |\tAux loss: 7674.98\n",
      "Train epoch 22: [160/318 (50%)]\tLoss: 8.609 |\tMSE loss: 0.008 |\tBpp loss: 3.56 |\tAux loss: 7669.81\n",
      "Test epoch 22: Average losses:\tLoss: 9.548 |\tMSE loss: 0.009 |\tBpp loss: 3.55 |\tAux loss: 7665.00\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 23: [0/318 (0%)]\tLoss: 10.074 |\tMSE loss: 0.010 |\tBpp loss: 3.55 |\tAux loss: 7664.45\n",
      "Train epoch 23: [160/318 (50%)]\tLoss: 9.144 |\tMSE loss: 0.009 |\tBpp loss: 3.54 |\tAux loss: 7658.97\n",
      "Test epoch 23: Average losses:\tLoss: 11.044 |\tMSE loss: 0.012 |\tBpp loss: 3.53 |\tAux loss: 7654.61\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 24: [0/318 (0%)]\tLoss: 13.272 |\tMSE loss: 0.015 |\tBpp loss: 3.53 |\tAux loss: 7654.19\n",
      "Train epoch 24: [160/318 (50%)]\tLoss: 12.922 |\tMSE loss: 0.014 |\tBpp loss: 3.52 |\tAux loss: 7649.66\n",
      "Test epoch 24: Average losses:\tLoss: 10.137 |\tMSE loss: 0.010 |\tBpp loss: 3.52 |\tAux loss: 7646.15\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 25: [0/318 (0%)]\tLoss: 11.831 |\tMSE loss: 0.013 |\tBpp loss: 3.52 |\tAux loss: 7645.86\n",
      "Train epoch 25: [160/318 (50%)]\tLoss: 8.910 |\tMSE loss: 0.008 |\tBpp loss: 3.51 |\tAux loss: 7642.23\n",
      "Test epoch 25: Average losses:\tLoss: 9.284 |\tMSE loss: 0.009 |\tBpp loss: 3.51 |\tAux loss: 7638.91\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 26: [0/318 (0%)]\tLoss: 6.406 |\tMSE loss: 0.004 |\tBpp loss: 3.51 |\tAux loss: 7638.65\n",
      "Train epoch 26: [160/318 (50%)]\tLoss: 9.122 |\tMSE loss: 0.009 |\tBpp loss: 3.50 |\tAux loss: 7635.08\n",
      "Test epoch 26: Average losses:\tLoss: 10.058 |\tMSE loss: 0.010 |\tBpp loss: 3.50 |\tAux loss: 7631.74\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 27: [0/318 (0%)]\tLoss: 9.801 |\tMSE loss: 0.010 |\tBpp loss: 3.49 |\tAux loss: 7631.46\n",
      "Train epoch 27: [160/318 (50%)]\tLoss: 9.941 |\tMSE loss: 0.010 |\tBpp loss: 3.49 |\tAux loss: 7627.73\n",
      "Test epoch 27: Average losses:\tLoss: 9.149 |\tMSE loss: 0.009 |\tBpp loss: 3.48 |\tAux loss: 7624.18\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 28: [0/318 (0%)]\tLoss: 8.895 |\tMSE loss: 0.008 |\tBpp loss: 3.48 |\tAux loss: 7623.88\n",
      "Train epoch 28: [160/318 (50%)]\tLoss: 9.319 |\tMSE loss: 0.009 |\tBpp loss: 3.48 |\tAux loss: 7620.05\n",
      "Test epoch 28: Average losses:\tLoss: 9.039 |\tMSE loss: 0.009 |\tBpp loss: 3.47 |\tAux loss: 7616.42\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 29: [0/318 (0%)]\tLoss: 7.137 |\tMSE loss: 0.006 |\tBpp loss: 3.47 |\tAux loss: 7616.11\n",
      "Train epoch 29: [160/318 (50%)]\tLoss: 7.551 |\tMSE loss: 0.006 |\tBpp loss: 3.47 |\tAux loss: 7612.14\n",
      "Test epoch 29: Average losses:\tLoss: 9.439 |\tMSE loss: 0.009 |\tBpp loss: 3.46 |\tAux loss: 7608.45\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 30: [0/318 (0%)]\tLoss: 10.505 |\tMSE loss: 0.011 |\tBpp loss: 3.46 |\tAux loss: 7608.14\n",
      "Train epoch 30: [160/318 (50%)]\tLoss: 10.185 |\tMSE loss: 0.010 |\tBpp loss: 3.45 |\tAux loss: 7603.97\n",
      "Test epoch 30: Average losses:\tLoss: 8.519 |\tMSE loss: 0.008 |\tBpp loss: 3.45 |\tAux loss: 7599.91\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 31: [0/318 (0%)]\tLoss: 10.701 |\tMSE loss: 0.011 |\tBpp loss: 3.45 |\tAux loss: 7599.50\n",
      "Train epoch 31: [160/318 (50%)]\tLoss: 7.158 |\tMSE loss: 0.006 |\tBpp loss: 3.44 |\tAux loss: 7594.58\n",
      "Test epoch 31: Average losses:\tLoss: 8.320 |\tMSE loss: 0.008 |\tBpp loss: 3.43 |\tAux loss: 7590.27\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 32: [0/318 (0%)]\tLoss: 7.903 |\tMSE loss: 0.007 |\tBpp loss: 3.43 |\tAux loss: 7589.81\n",
      "Train epoch 32: [160/318 (50%)]\tLoss: 8.284 |\tMSE loss: 0.007 |\tBpp loss: 3.42 |\tAux loss: 7583.16\n",
      "Test epoch 32: Average losses:\tLoss: 7.646 |\tMSE loss: 0.007 |\tBpp loss: 3.41 |\tAux loss: 7576.77\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 33: [0/318 (0%)]\tLoss: 8.295 |\tMSE loss: 0.008 |\tBpp loss: 3.41 |\tAux loss: 7576.22\n",
      "Train epoch 33: [160/318 (50%)]\tLoss: 8.519 |\tMSE loss: 0.008 |\tBpp loss: 3.40 |\tAux loss: 7568.44\n",
      "Test epoch 33: Average losses:\tLoss: 8.039 |\tMSE loss: 0.007 |\tBpp loss: 3.38 |\tAux loss: 7560.98\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 34: [0/318 (0%)]\tLoss: 7.805 |\tMSE loss: 0.007 |\tBpp loss: 3.38 |\tAux loss: 7560.31\n",
      "Train epoch 34: [160/318 (50%)]\tLoss: 10.351 |\tMSE loss: 0.011 |\tBpp loss: 3.37 |\tAux loss: 7552.51\n",
      "Test epoch 34: Average losses:\tLoss: 8.884 |\tMSE loss: 0.009 |\tBpp loss: 3.35 |\tAux loss: 7543.81\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 35: [0/318 (0%)]\tLoss: 7.829 |\tMSE loss: 0.007 |\tBpp loss: 3.35 |\tAux loss: 7543.01\n",
      "Train epoch 35: [160/318 (50%)]\tLoss: 8.940 |\tMSE loss: 0.009 |\tBpp loss: 3.34 |\tAux loss: 7535.22\n",
      "Test epoch 35: Average losses:\tLoss: 7.650 |\tMSE loss: 0.007 |\tBpp loss: 3.33 |\tAux loss: 7529.10\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 36: [0/318 (0%)]\tLoss: 10.802 |\tMSE loss: 0.011 |\tBpp loss: 3.33 |\tAux loss: 7528.58\n",
      "Train epoch 36: [160/318 (50%)]\tLoss: 6.450 |\tMSE loss: 0.005 |\tBpp loss: 3.32 |\tAux loss: 7522.76\n",
      "Test epoch 36: Average losses:\tLoss: 7.320 |\tMSE loss: 0.006 |\tBpp loss: 3.31 |\tAux loss: 7517.48\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 37: [0/318 (0%)]\tLoss: 6.251 |\tMSE loss: 0.005 |\tBpp loss: 3.31 |\tAux loss: 7516.93\n",
      "Train epoch 37: [160/318 (50%)]\tLoss: 9.733 |\tMSE loss: 0.010 |\tBpp loss: 3.30 |\tAux loss: 7510.56\n",
      "Test epoch 37: Average losses:\tLoss: 7.377 |\tMSE loss: 0.006 |\tBpp loss: 3.29 |\tAux loss: 7504.25\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 38: [0/318 (0%)]\tLoss: 6.483 |\tMSE loss: 0.005 |\tBpp loss: 3.29 |\tAux loss: 7503.69\n",
      "Train epoch 38: [160/318 (50%)]\tLoss: 7.222 |\tMSE loss: 0.006 |\tBpp loss: 3.28 |\tAux loss: 7497.15\n",
      "Test epoch 38: Average losses:\tLoss: 7.353 |\tMSE loss: 0.006 |\tBpp loss: 3.27 |\tAux loss: 7491.55\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 39: [0/318 (0%)]\tLoss: 7.228 |\tMSE loss: 0.006 |\tBpp loss: 3.28 |\tAux loss: 7491.06\n",
      "Train epoch 39: [160/318 (50%)]\tLoss: 8.616 |\tMSE loss: 0.008 |\tBpp loss: 3.26 |\tAux loss: 7484.38\n",
      "Test epoch 39: Average losses:\tLoss: 9.075 |\tMSE loss: 0.009 |\tBpp loss: 3.25 |\tAux loss: 7477.78\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 40: [0/318 (0%)]\tLoss: 8.195 |\tMSE loss: 0.008 |\tBpp loss: 3.25 |\tAux loss: 7477.23\n",
      "Train epoch 40: [160/318 (50%)]\tLoss: 10.679 |\tMSE loss: 0.011 |\tBpp loss: 3.24 |\tAux loss: 7470.86\n",
      "Test epoch 40: Average losses:\tLoss: 8.194 |\tMSE loss: 0.008 |\tBpp loss: 3.23 |\tAux loss: 7463.56\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 41: [0/318 (0%)]\tLoss: 7.002 |\tMSE loss: 0.006 |\tBpp loss: 3.23 |\tAux loss: 7462.91\n",
      "Train epoch 41: [160/318 (50%)]\tLoss: 9.676 |\tMSE loss: 0.010 |\tBpp loss: 3.22 |\tAux loss: 7455.46\n",
      "Test epoch 41: Average losses:\tLoss: 7.935 |\tMSE loss: 0.007 |\tBpp loss: 3.21 |\tAux loss: 7448.46\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 42: [0/318 (0%)]\tLoss: 6.002 |\tMSE loss: 0.004 |\tBpp loss: 3.21 |\tAux loss: 7447.67\n",
      "Train epoch 42: [160/318 (50%)]\tLoss: 8.430 |\tMSE loss: 0.008 |\tBpp loss: 3.20 |\tAux loss: 7440.30\n",
      "Test epoch 42: Average losses:\tLoss: 7.203 |\tMSE loss: 0.006 |\tBpp loss: 3.19 |\tAux loss: 7434.54\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 43: [0/318 (0%)]\tLoss: 7.150 |\tMSE loss: 0.006 |\tBpp loss: 3.19 |\tAux loss: 7434.04\n",
      "Train epoch 43: [160/318 (50%)]\tLoss: 9.146 |\tMSE loss: 0.009 |\tBpp loss: 3.18 |\tAux loss: 7427.72\n",
      "Test epoch 43: Average losses:\tLoss: 7.393 |\tMSE loss: 0.006 |\tBpp loss: 3.17 |\tAux loss: 7422.31\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 44: [0/318 (0%)]\tLoss: 6.658 |\tMSE loss: 0.005 |\tBpp loss: 3.17 |\tAux loss: 7421.88\n",
      "Train epoch 44: [160/318 (50%)]\tLoss: 6.630 |\tMSE loss: 0.005 |\tBpp loss: 3.16 |\tAux loss: 7414.62\n",
      "Test epoch 44: Average losses:\tLoss: 7.190 |\tMSE loss: 0.006 |\tBpp loss: 3.15 |\tAux loss: 7407.92\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 45: [0/318 (0%)]\tLoss: 8.052 |\tMSE loss: 0.008 |\tBpp loss: 3.15 |\tAux loss: 7407.42\n",
      "Train epoch 45: [160/318 (50%)]\tLoss: 8.316 |\tMSE loss: 0.008 |\tBpp loss: 3.14 |\tAux loss: 7399.41\n",
      "Test epoch 45: Average losses:\tLoss: 7.465 |\tMSE loss: 0.007 |\tBpp loss: 3.12 |\tAux loss: 7390.69\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 46: [0/318 (0%)]\tLoss: 7.120 |\tMSE loss: 0.006 |\tBpp loss: 3.12 |\tAux loss: 7389.93\n",
      "Train epoch 46: [160/318 (50%)]\tLoss: 5.920 |\tMSE loss: 0.004 |\tBpp loss: 3.11 |\tAux loss: 7381.96\n",
      "Test epoch 46: Average losses:\tLoss: 6.986 |\tMSE loss: 0.006 |\tBpp loss: 3.10 |\tAux loss: 7374.09\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 47: [0/318 (0%)]\tLoss: 7.530 |\tMSE loss: 0.007 |\tBpp loss: 3.10 |\tAux loss: 7373.31\n",
      "Train epoch 47: [160/318 (50%)]\tLoss: 6.469 |\tMSE loss: 0.005 |\tBpp loss: 3.09 |\tAux loss: 7365.15\n",
      "Test epoch 47: Average losses:\tLoss: 6.459 |\tMSE loss: 0.005 |\tBpp loss: 3.07 |\tAux loss: 7358.18\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 48: [0/318 (0%)]\tLoss: 6.371 |\tMSE loss: 0.005 |\tBpp loss: 3.07 |\tAux loss: 7357.61\n",
      "Train epoch 48: [160/318 (50%)]\tLoss: 5.268 |\tMSE loss: 0.003 |\tBpp loss: 3.07 |\tAux loss: 7351.10\n",
      "Test epoch 48: Average losses:\tLoss: 6.504 |\tMSE loss: 0.005 |\tBpp loss: 3.06 |\tAux loss: 7345.50\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 49: [0/318 (0%)]\tLoss: 5.385 |\tMSE loss: 0.004 |\tBpp loss: 3.06 |\tAux loss: 7345.04\n",
      "Train epoch 49: [160/318 (50%)]\tLoss: 6.931 |\tMSE loss: 0.006 |\tBpp loss: 3.05 |\tAux loss: 7339.20\n",
      "Test epoch 49: Average losses:\tLoss: 6.379 |\tMSE loss: 0.005 |\tBpp loss: 3.04 |\tAux loss: 7333.32\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 50: [0/318 (0%)]\tLoss: 5.506 |\tMSE loss: 0.004 |\tBpp loss: 3.04 |\tAux loss: 7332.85\n",
      "Train epoch 50: [160/318 (50%)]\tLoss: 5.826 |\tMSE loss: 0.004 |\tBpp loss: 3.04 |\tAux loss: 7327.02\n",
      "Test epoch 50: Average losses:\tLoss: 6.886 |\tMSE loss: 0.006 |\tBpp loss: 3.03 |\tAux loss: 7321.50\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 51: [0/318 (0%)]\tLoss: 6.600 |\tMSE loss: 0.005 |\tBpp loss: 3.03 |\tAux loss: 7321.00\n",
      "Train epoch 51: [160/318 (50%)]\tLoss: 6.624 |\tMSE loss: 0.006 |\tBpp loss: 3.02 |\tAux loss: 7315.24\n",
      "Test epoch 51: Average losses:\tLoss: 6.582 |\tMSE loss: 0.005 |\tBpp loss: 3.01 |\tAux loss: 7309.81\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 52: [0/318 (0%)]\tLoss: 7.742 |\tMSE loss: 0.007 |\tBpp loss: 3.02 |\tAux loss: 7309.37\n",
      "Train epoch 52: [160/318 (50%)]\tLoss: 6.370 |\tMSE loss: 0.005 |\tBpp loss: 3.01 |\tAux loss: 7303.38\n",
      "Test epoch 52: Average losses:\tLoss: 6.451 |\tMSE loss: 0.005 |\tBpp loss: 2.99 |\tAux loss: 7296.75\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 53: [0/318 (0%)]\tLoss: 6.175 |\tMSE loss: 0.005 |\tBpp loss: 3.00 |\tAux loss: 7296.19\n",
      "Train epoch 53: [160/318 (50%)]\tLoss: 8.175 |\tMSE loss: 0.008 |\tBpp loss: 2.99 |\tAux loss: 7289.46\n",
      "Test epoch 53: Average losses:\tLoss: 6.422 |\tMSE loss: 0.005 |\tBpp loss: 2.98 |\tAux loss: 7283.73\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 54: [0/318 (0%)]\tLoss: 6.155 |\tMSE loss: 0.005 |\tBpp loss: 2.98 |\tAux loss: 7283.25\n",
      "Train epoch 54: [160/318 (50%)]\tLoss: 6.671 |\tMSE loss: 0.006 |\tBpp loss: 2.97 |\tAux loss: 7277.44\n",
      "Test epoch 54: Average losses:\tLoss: 6.630 |\tMSE loss: 0.006 |\tBpp loss: 2.96 |\tAux loss: 7272.42\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 55: [0/318 (0%)]\tLoss: 5.693 |\tMSE loss: 0.004 |\tBpp loss: 2.97 |\tAux loss: 7271.96\n",
      "Train epoch 55: [160/318 (50%)]\tLoss: 5.923 |\tMSE loss: 0.005 |\tBpp loss: 2.96 |\tAux loss: 7265.06\n",
      "Test epoch 55: Average losses:\tLoss: 6.529 |\tMSE loss: 0.006 |\tBpp loss: 2.94 |\tAux loss: 7257.82\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 56: [0/318 (0%)]\tLoss: 6.829 |\tMSE loss: 0.006 |\tBpp loss: 2.95 |\tAux loss: 7257.19\n",
      "Train epoch 56: [160/318 (50%)]\tLoss: 5.328 |\tMSE loss: 0.004 |\tBpp loss: 2.94 |\tAux loss: 7250.32\n",
      "Test epoch 56: Average losses:\tLoss: 6.323 |\tMSE loss: 0.005 |\tBpp loss: 2.93 |\tAux loss: 7243.92\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 57: [0/318 (0%)]\tLoss: 6.122 |\tMSE loss: 0.005 |\tBpp loss: 2.93 |\tAux loss: 7243.36\n",
      "Train epoch 57: [160/318 (50%)]\tLoss: 5.656 |\tMSE loss: 0.004 |\tBpp loss: 2.92 |\tAux loss: 7236.09\n",
      "Test epoch 57: Average losses:\tLoss: 6.579 |\tMSE loss: 0.006 |\tBpp loss: 2.91 |\tAux loss: 7229.33\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 58: [0/318 (0%)]\tLoss: 6.106 |\tMSE loss: 0.005 |\tBpp loss: 2.91 |\tAux loss: 7228.71\n",
      "Train epoch 58: [160/318 (50%)]\tLoss: 7.414 |\tMSE loss: 0.007 |\tBpp loss: 2.90 |\tAux loss: 7221.45\n",
      "Test epoch 58: Average losses:\tLoss: 6.421 |\tMSE loss: 0.005 |\tBpp loss: 2.89 |\tAux loss: 7214.07\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 59: [0/318 (0%)]\tLoss: 5.852 |\tMSE loss: 0.005 |\tBpp loss: 2.89 |\tAux loss: 7213.26\n",
      "Train epoch 59: [160/318 (50%)]\tLoss: 6.025 |\tMSE loss: 0.005 |\tBpp loss: 2.88 |\tAux loss: 7204.37\n",
      "Test epoch 59: Average losses:\tLoss: 6.297 |\tMSE loss: 0.005 |\tBpp loss: 2.87 |\tAux loss: 7195.04\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 60: [0/318 (0%)]\tLoss: 7.090 |\tMSE loss: 0.006 |\tBpp loss: 2.87 |\tAux loss: 7194.25\n",
      "Train epoch 60: [160/318 (50%)]\tLoss: 6.135 |\tMSE loss: 0.005 |\tBpp loss: 2.86 |\tAux loss: 7185.95\n",
      "Test epoch 60: Average losses:\tLoss: 7.513 |\tMSE loss: 0.007 |\tBpp loss: 2.85 |\tAux loss: 7179.32\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 61: [0/318 (0%)]\tLoss: 8.169 |\tMSE loss: 0.008 |\tBpp loss: 2.85 |\tAux loss: 7178.79\n",
      "Train epoch 61: [160/318 (50%)]\tLoss: 6.172 |\tMSE loss: 0.005 |\tBpp loss: 2.84 |\tAux loss: 7172.34\n",
      "Test epoch 61: Average losses:\tLoss: 5.939 |\tMSE loss: 0.005 |\tBpp loss: 2.83 |\tAux loss: 7166.15\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 62: [0/318 (0%)]\tLoss: 5.862 |\tMSE loss: 0.005 |\tBpp loss: 2.83 |\tAux loss: 7165.60\n",
      "Train epoch 62: [160/318 (50%)]\tLoss: 6.897 |\tMSE loss: 0.006 |\tBpp loss: 2.83 |\tAux loss: 7159.22\n",
      "Test epoch 62: Average losses:\tLoss: 5.755 |\tMSE loss: 0.005 |\tBpp loss: 2.82 |\tAux loss: 7153.43\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 63: [0/318 (0%)]\tLoss: 4.333 |\tMSE loss: 0.002 |\tBpp loss: 2.82 |\tAux loss: 7152.89\n",
      "Train epoch 63: [160/318 (50%)]\tLoss: 6.976 |\tMSE loss: 0.006 |\tBpp loss: 2.82 |\tAux loss: 7146.44\n",
      "Test epoch 63: Average losses:\tLoss: 5.842 |\tMSE loss: 0.005 |\tBpp loss: 2.80 |\tAux loss: 7140.68\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 64: [0/318 (0%)]\tLoss: 4.722 |\tMSE loss: 0.003 |\tBpp loss: 2.81 |\tAux loss: 7140.19\n",
      "Train epoch 64: [160/318 (50%)]\tLoss: 4.424 |\tMSE loss: 0.002 |\tBpp loss: 2.80 |\tAux loss: 7133.62\n",
      "Test epoch 64: Average losses:\tLoss: 5.845 |\tMSE loss: 0.005 |\tBpp loss: 2.79 |\tAux loss: 7125.82\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 65: [0/318 (0%)]\tLoss: 5.167 |\tMSE loss: 0.004 |\tBpp loss: 2.80 |\tAux loss: 7124.86\n",
      "Train epoch 65: [160/318 (50%)]\tLoss: 6.202 |\tMSE loss: 0.005 |\tBpp loss: 2.78 |\tAux loss: 7113.84\n",
      "Test epoch 65: Average losses:\tLoss: 5.815 |\tMSE loss: 0.005 |\tBpp loss: 2.76 |\tAux loss: 7106.20\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 66: [0/318 (0%)]\tLoss: 5.036 |\tMSE loss: 0.003 |\tBpp loss: 2.77 |\tAux loss: 7105.53\n",
      "Train epoch 66: [160/318 (50%)]\tLoss: 6.319 |\tMSE loss: 0.005 |\tBpp loss: 2.76 |\tAux loss: 7096.67\n",
      "Test epoch 66: Average losses:\tLoss: 5.940 |\tMSE loss: 0.005 |\tBpp loss: 2.75 |\tAux loss: 7089.03\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 67: [0/318 (0%)]\tLoss: 5.347 |\tMSE loss: 0.004 |\tBpp loss: 2.75 |\tAux loss: 7088.42\n",
      "Train epoch 67: [160/318 (50%)]\tLoss: 4.783 |\tMSE loss: 0.003 |\tBpp loss: 2.74 |\tAux loss: 7081.32\n",
      "Test epoch 67: Average losses:\tLoss: 5.862 |\tMSE loss: 0.005 |\tBpp loss: 2.73 |\tAux loss: 7075.36\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 68: [0/318 (0%)]\tLoss: 5.485 |\tMSE loss: 0.004 |\tBpp loss: 2.73 |\tAux loss: 7074.86\n",
      "Train epoch 68: [160/318 (50%)]\tLoss: 5.475 |\tMSE loss: 0.004 |\tBpp loss: 2.73 |\tAux loss: 7068.70\n",
      "Test epoch 68: Average losses:\tLoss: 5.938 |\tMSE loss: 0.005 |\tBpp loss: 2.72 |\tAux loss: 7062.99\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 69: [0/318 (0%)]\tLoss: 4.957 |\tMSE loss: 0.003 |\tBpp loss: 2.72 |\tAux loss: 7062.52\n",
      "Train epoch 69: [160/318 (50%)]\tLoss: 5.648 |\tMSE loss: 0.005 |\tBpp loss: 2.72 |\tAux loss: 7056.24\n",
      "Test epoch 69: Average losses:\tLoss: 5.838 |\tMSE loss: 0.005 |\tBpp loss: 2.70 |\tAux loss: 7049.93\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 70: [0/318 (0%)]\tLoss: 5.911 |\tMSE loss: 0.005 |\tBpp loss: 2.70 |\tAux loss: 7049.37\n",
      "Train epoch 70: [160/318 (50%)]\tLoss: 6.178 |\tMSE loss: 0.005 |\tBpp loss: 2.70 |\tAux loss: 7042.35\n",
      "Test epoch 70: Average losses:\tLoss: 5.727 |\tMSE loss: 0.005 |\tBpp loss: 2.69 |\tAux loss: 7035.42\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 71: [0/318 (0%)]\tLoss: 5.343 |\tMSE loss: 0.004 |\tBpp loss: 2.69 |\tAux loss: 7034.83\n",
      "Train epoch 71: [160/318 (50%)]\tLoss: 4.509 |\tMSE loss: 0.003 |\tBpp loss: 2.68 |\tAux loss: 7027.43\n",
      "Test epoch 71: Average losses:\tLoss: 5.538 |\tMSE loss: 0.004 |\tBpp loss: 2.68 |\tAux loss: 7021.01\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 72: [0/318 (0%)]\tLoss: 4.539 |\tMSE loss: 0.003 |\tBpp loss: 2.68 |\tAux loss: 7020.48\n",
      "Train epoch 72: [160/318 (50%)]\tLoss: 6.937 |\tMSE loss: 0.007 |\tBpp loss: 2.67 |\tAux loss: 7013.27\n",
      "Test epoch 72: Average losses:\tLoss: 6.171 |\tMSE loss: 0.005 |\tBpp loss: 2.66 |\tAux loss: 7007.12\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 73: [0/318 (0%)]\tLoss: 6.692 |\tMSE loss: 0.006 |\tBpp loss: 2.67 |\tAux loss: 7006.67\n",
      "Train epoch 73: [160/318 (50%)]\tLoss: 4.615 |\tMSE loss: 0.003 |\tBpp loss: 2.66 |\tAux loss: 7001.16\n",
      "Test epoch 73: Average losses:\tLoss: 6.094 |\tMSE loss: 0.005 |\tBpp loss: 2.65 |\tAux loss: 6996.59\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 74: [0/318 (0%)]\tLoss: 5.975 |\tMSE loss: 0.005 |\tBpp loss: 2.66 |\tAux loss: 6996.26\n",
      "Train epoch 74: [160/318 (50%)]\tLoss: 6.759 |\tMSE loss: 0.006 |\tBpp loss: 2.65 |\tAux loss: 6991.72\n",
      "Test epoch 74: Average losses:\tLoss: 5.547 |\tMSE loss: 0.004 |\tBpp loss: 2.64 |\tAux loss: 6987.58\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 75: [0/318 (0%)]\tLoss: 5.125 |\tMSE loss: 0.004 |\tBpp loss: 2.65 |\tAux loss: 6987.25\n",
      "Train epoch 75: [160/318 (50%)]\tLoss: 6.540 |\tMSE loss: 0.006 |\tBpp loss: 2.65 |\tAux loss: 6982.70\n",
      "Test epoch 75: Average losses:\tLoss: 6.148 |\tMSE loss: 0.005 |\tBpp loss: 2.64 |\tAux loss: 6978.41\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 76: [0/318 (0%)]\tLoss: 6.257 |\tMSE loss: 0.006 |\tBpp loss: 2.64 |\tAux loss: 6978.10\n",
      "Train epoch 76: [160/318 (50%)]\tLoss: 4.231 |\tMSE loss: 0.002 |\tBpp loss: 2.63 |\tAux loss: 6973.52\n",
      "Test epoch 76: Average losses:\tLoss: 5.560 |\tMSE loss: 0.005 |\tBpp loss: 2.63 |\tAux loss: 6968.91\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 77: [0/318 (0%)]\tLoss: 4.505 |\tMSE loss: 0.003 |\tBpp loss: 2.63 |\tAux loss: 6968.52\n",
      "Train epoch 77: [160/318 (50%)]\tLoss: 6.323 |\tMSE loss: 0.006 |\tBpp loss: 2.62 |\tAux loss: 6963.41\n",
      "Test epoch 77: Average losses:\tLoss: 5.578 |\tMSE loss: 0.005 |\tBpp loss: 2.62 |\tAux loss: 6958.82\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 78: [0/318 (0%)]\tLoss: 5.483 |\tMSE loss: 0.004 |\tBpp loss: 2.63 |\tAux loss: 6958.44\n",
      "Train epoch 78: [160/318 (50%)]\tLoss: 7.468 |\tMSE loss: 0.007 |\tBpp loss: 2.62 |\tAux loss: 6953.52\n",
      "Test epoch 78: Average losses:\tLoss: 6.133 |\tMSE loss: 0.005 |\tBpp loss: 2.61 |\tAux loss: 6948.75\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 79: [0/318 (0%)]\tLoss: 6.359 |\tMSE loss: 0.006 |\tBpp loss: 2.61 |\tAux loss: 6948.37\n",
      "Train epoch 79: [160/318 (50%)]\tLoss: 5.590 |\tMSE loss: 0.005 |\tBpp loss: 2.61 |\tAux loss: 6943.08\n",
      "Test epoch 79: Average losses:\tLoss: 5.794 |\tMSE loss: 0.005 |\tBpp loss: 2.60 |\tAux loss: 6938.28\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 80: [0/318 (0%)]\tLoss: 5.971 |\tMSE loss: 0.005 |\tBpp loss: 2.61 |\tAux loss: 6937.92\n",
      "Train epoch 80: [160/318 (50%)]\tLoss: 5.460 |\tMSE loss: 0.004 |\tBpp loss: 2.60 |\tAux loss: 6932.89\n",
      "Test epoch 80: Average losses:\tLoss: 5.380 |\tMSE loss: 0.004 |\tBpp loss: 2.59 |\tAux loss: 6928.17\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 81: [0/318 (0%)]\tLoss: 5.112 |\tMSE loss: 0.004 |\tBpp loss: 2.60 |\tAux loss: 6927.74\n",
      "Train epoch 81: [160/318 (50%)]\tLoss: 5.099 |\tMSE loss: 0.004 |\tBpp loss: 2.59 |\tAux loss: 6922.54\n",
      "Test epoch 81: Average losses:\tLoss: 6.330 |\tMSE loss: 0.006 |\tBpp loss: 2.58 |\tAux loss: 6917.53\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 82: [0/318 (0%)]\tLoss: 6.282 |\tMSE loss: 0.006 |\tBpp loss: 2.59 |\tAux loss: 6917.11\n",
      "Train epoch 82: [160/318 (50%)]\tLoss: 3.826 |\tMSE loss: 0.002 |\tBpp loss: 2.58 |\tAux loss: 6911.24\n",
      "Test epoch 82: Average losses:\tLoss: 5.529 |\tMSE loss: 0.005 |\tBpp loss: 2.57 |\tAux loss: 6906.13\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 83: [0/318 (0%)]\tLoss: 6.079 |\tMSE loss: 0.005 |\tBpp loss: 2.58 |\tAux loss: 6905.73\n",
      "Train epoch 83: [160/318 (50%)]\tLoss: 6.333 |\tMSE loss: 0.006 |\tBpp loss: 2.58 |\tAux loss: 6900.63\n",
      "Test epoch 83: Average losses:\tLoss: 5.387 |\tMSE loss: 0.004 |\tBpp loss: 2.57 |\tAux loss: 6895.80\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 84: [0/318 (0%)]\tLoss: 5.308 |\tMSE loss: 0.004 |\tBpp loss: 2.56 |\tAux loss: 6895.37\n",
      "Train epoch 84: [160/318 (50%)]\tLoss: 6.343 |\tMSE loss: 0.006 |\tBpp loss: 2.57 |\tAux loss: 6890.02\n",
      "Test epoch 84: Average losses:\tLoss: 5.877 |\tMSE loss: 0.005 |\tBpp loss: 2.56 |\tAux loss: 6885.25\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 85: [0/318 (0%)]\tLoss: 8.047 |\tMSE loss: 0.008 |\tBpp loss: 2.56 |\tAux loss: 6884.89\n",
      "Train epoch 85: [160/318 (50%)]\tLoss: 6.380 |\tMSE loss: 0.006 |\tBpp loss: 2.56 |\tAux loss: 6879.85\n",
      "Test epoch 85: Average losses:\tLoss: 5.462 |\tMSE loss: 0.004 |\tBpp loss: 2.55 |\tAux loss: 6874.94\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 86: [0/318 (0%)]\tLoss: 4.507 |\tMSE loss: 0.003 |\tBpp loss: 2.55 |\tAux loss: 6874.51\n",
      "Train epoch 86: [160/318 (50%)]\tLoss: 7.488 |\tMSE loss: 0.008 |\tBpp loss: 2.55 |\tAux loss: 6868.97\n",
      "Test epoch 86: Average losses:\tLoss: 5.547 |\tMSE loss: 0.005 |\tBpp loss: 2.54 |\tAux loss: 6863.96\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 87: [0/318 (0%)]\tLoss: 6.666 |\tMSE loss: 0.006 |\tBpp loss: 2.54 |\tAux loss: 6863.55\n",
      "Train epoch 87: [160/318 (50%)]\tLoss: 4.364 |\tMSE loss: 0.003 |\tBpp loss: 2.54 |\tAux loss: 6858.03\n",
      "Test epoch 87: Average losses:\tLoss: 5.899 |\tMSE loss: 0.005 |\tBpp loss: 2.53 |\tAux loss: 6852.67\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 88: [0/318 (0%)]\tLoss: 6.012 |\tMSE loss: 0.005 |\tBpp loss: 2.53 |\tAux loss: 6852.22\n",
      "Train epoch 88: [160/318 (50%)]\tLoss: 5.356 |\tMSE loss: 0.004 |\tBpp loss: 2.53 |\tAux loss: 6846.43\n",
      "Test epoch 88: Average losses:\tLoss: 5.543 |\tMSE loss: 0.005 |\tBpp loss: 2.52 |\tAux loss: 6841.09\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 89: [0/318 (0%)]\tLoss: 5.538 |\tMSE loss: 0.005 |\tBpp loss: 2.53 |\tAux loss: 6840.64\n",
      "Train epoch 89: [160/318 (50%)]\tLoss: 6.084 |\tMSE loss: 0.005 |\tBpp loss: 2.52 |\tAux loss: 6834.86\n",
      "Test epoch 89: Average losses:\tLoss: 5.350 |\tMSE loss: 0.004 |\tBpp loss: 2.51 |\tAux loss: 6829.69\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 90: [0/318 (0%)]\tLoss: 4.536 |\tMSE loss: 0.003 |\tBpp loss: 2.51 |\tAux loss: 6829.24\n",
      "Train epoch 90: [160/318 (50%)]\tLoss: 5.405 |\tMSE loss: 0.004 |\tBpp loss: 2.50 |\tAux loss: 6823.54\n",
      "Test epoch 90: Average losses:\tLoss: 6.122 |\tMSE loss: 0.006 |\tBpp loss: 2.50 |\tAux loss: 6818.27\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 91: [0/318 (0%)]\tLoss: 5.189 |\tMSE loss: 0.004 |\tBpp loss: 2.50 |\tAux loss: 6817.83\n",
      "Train epoch 91: [160/318 (50%)]\tLoss: 5.636 |\tMSE loss: 0.005 |\tBpp loss: 2.50 |\tAux loss: 6812.11\n",
      "Test epoch 91: Average losses:\tLoss: 5.489 |\tMSE loss: 0.005 |\tBpp loss: 2.49 |\tAux loss: 6806.41\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 92: [0/318 (0%)]\tLoss: 5.048 |\tMSE loss: 0.004 |\tBpp loss: 2.49 |\tAux loss: 6805.88\n",
      "Train epoch 92: [160/318 (50%)]\tLoss: 4.804 |\tMSE loss: 0.004 |\tBpp loss: 2.48 |\tAux loss: 6799.50\n",
      "Test epoch 92: Average losses:\tLoss: 5.498 |\tMSE loss: 0.005 |\tBpp loss: 2.48 |\tAux loss: 6793.84\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 93: [0/318 (0%)]\tLoss: 5.460 |\tMSE loss: 0.005 |\tBpp loss: 2.49 |\tAux loss: 6793.33\n",
      "Train epoch 93: [160/318 (50%)]\tLoss: 4.262 |\tMSE loss: 0.003 |\tBpp loss: 2.47 |\tAux loss: 6787.31\n",
      "Test epoch 93: Average losses:\tLoss: 5.840 |\tMSE loss: 0.005 |\tBpp loss: 2.47 |\tAux loss: 6781.71\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 94: [0/318 (0%)]\tLoss: 5.998 |\tMSE loss: 0.005 |\tBpp loss: 2.48 |\tAux loss: 6781.22\n",
      "Train epoch 94: [160/318 (50%)]\tLoss: 5.807 |\tMSE loss: 0.005 |\tBpp loss: 2.47 |\tAux loss: 6774.98\n",
      "Test epoch 94: Average losses:\tLoss: 5.639 |\tMSE loss: 0.005 |\tBpp loss: 2.46 |\tAux loss: 6769.09\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 95: [0/318 (0%)]\tLoss: 5.111 |\tMSE loss: 0.004 |\tBpp loss: 2.46 |\tAux loss: 6768.55\n",
      "Train epoch 95: [160/318 (50%)]\tLoss: 4.790 |\tMSE loss: 0.004 |\tBpp loss: 2.46 |\tAux loss: 6762.15\n",
      "Test epoch 95: Average losses:\tLoss: 5.041 |\tMSE loss: 0.004 |\tBpp loss: 2.45 |\tAux loss: 6756.05\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 96: [0/318 (0%)]\tLoss: 5.201 |\tMSE loss: 0.004 |\tBpp loss: 2.46 |\tAux loss: 6755.40\n",
      "Train epoch 96: [160/318 (50%)]\tLoss: 4.874 |\tMSE loss: 0.004 |\tBpp loss: 2.44 |\tAux loss: 6748.21\n",
      "Test epoch 96: Average losses:\tLoss: 5.932 |\tMSE loss: 0.005 |\tBpp loss: 2.43 |\tAux loss: 6741.60\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 97: [0/318 (0%)]\tLoss: 6.247 |\tMSE loss: 0.006 |\tBpp loss: 2.44 |\tAux loss: 6741.05\n",
      "Train epoch 97: [160/318 (50%)]\tLoss: 4.902 |\tMSE loss: 0.004 |\tBpp loss: 2.44 |\tAux loss: 6734.41\n",
      "Test epoch 97: Average losses:\tLoss: 5.274 |\tMSE loss: 0.004 |\tBpp loss: 2.43 |\tAux loss: 6728.61\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 98: [0/318 (0%)]\tLoss: 4.538 |\tMSE loss: 0.003 |\tBpp loss: 2.43 |\tAux loss: 6728.06\n",
      "Train epoch 98: [160/318 (50%)]\tLoss: 5.320 |\tMSE loss: 0.004 |\tBpp loss: 2.42 |\tAux loss: 6721.47\n",
      "Test epoch 98: Average losses:\tLoss: 5.085 |\tMSE loss: 0.004 |\tBpp loss: 2.42 |\tAux loss: 6715.29\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 99: [0/318 (0%)]\tLoss: 3.629 |\tMSE loss: 0.002 |\tBpp loss: 2.41 |\tAux loss: 6714.69\n",
      "Train epoch 99: [160/318 (50%)]\tLoss: 4.726 |\tMSE loss: 0.004 |\tBpp loss: 2.42 |\tAux loss: 6707.56\n",
      "Test epoch 99: Average losses:\tLoss: 5.202 |\tMSE loss: 0.004 |\tBpp loss: 2.40 |\tAux loss: 6700.86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(last_epoch, epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    train_one_epoch(\n",
    "        net,\n",
    "        criterion,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        aux_optimizer,\n",
    "        epoch,\n",
    "        clip_max_norm,\n",
    "    )\n",
    "    loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
    "    lr_scheduler.step(loss)\n",
    "\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    if save:\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": net.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            },\n",
    "            is_best,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
