{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f99360-2a08-41d5-9ded-b7912486fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from compressai.datasets import ImageFolder\n",
    "from compressai.zoo import image_models\n",
    "from compressai.optimizers import net_aux_optimizer\n",
    "from compressai.losses import RateDistortionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33133bc-1595-428c-b264-e7ab306cbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='bmshj2018-factorized'\n",
    "dataset='/home/server/datasets/example'\n",
    "epochs=100\n",
    "learning_rate=0.0001\n",
    "num_workers=1\n",
    "lmbda=0.01\n",
    "batch_size=16\n",
    "test_batch_size=64\n",
    "aux_learning_rate=0.001\n",
    "patch_size=(256, 256)\n",
    "cuda=True\n",
    "save=True\n",
    "seed=0\n",
    "clip_max_norm=1.0\n",
    "checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948163ad-5683-4762-9c25-a1efeb30dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(net):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "    conf = {\n",
    "        \"net\": {\"type\": \"Adam\", \"lr\": learning_rate},\n",
    "        \"aux\": {\"type\": \"Adam\", \"lr\": aux_learning_rate},\n",
    "    }\n",
    "    optimizer = net_aux_optimizer(net, conf)\n",
    "    return optimizer[\"net\"], optimizer[\"aux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7dec53-a0d3-4614-afac-8c5ef9efa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "\n",
    "        out_net = model(d)\n",
    "\n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward()\n",
    "        if clip_max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\tMSE loss: {out_criterion[\"mse_loss\"].item():.3f} |'\n",
    "                f'\\tBpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'\n",
    "                f\"\\tAux loss: {aux_loss.item():.2f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75f414e-0002-49c2-b370-247a4b646c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(epoch, test_dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    mse_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "            out_net = model(d)\n",
    "            out_criterion = criterion(out_net, d)\n",
    "\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
    "\n",
    "    print(\n",
    "        f\"Test epoch {epoch}: Average losses:\"\n",
    "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "        f\"\\tMSE loss: {mse_loss.avg:.3f} |\"\n",
    "        f\"\\tBpp loss: {bpp_loss.avg:.2f} |\"\n",
    "        f\"\\tAux loss: {aux_loss.avg:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    return loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5315e0-20e2-46e1-bf6f-7dc6170ca724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5972b3b-5374-4d0a-8388-1774f8b06a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"checkpoint_best_loss.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c19ca3-9318-4837-b59c-754911dc26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder(dataset, split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder(dataset, split=\"test\", transform=test_transforms)\n",
    "\n",
    "device = \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "net = image_models[model](quality=3)\n",
    "net = net.to(device)\n",
    "optimizer, aux_optimizer = configure_optimizers(net)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "criterion = RateDistortionLoss(lmbda=lmbda)\n",
    "last_epoch = 0\n",
    "\n",
    "if checkpoint:  # load from previous checkpoint\n",
    "    print(\"Loading\", checkpoint)\n",
    "    checkpoint = torch.load(checkpoint, map_location=device)\n",
    "    last_epoch = checkpoint[\"epoch\"] + 1\n",
    "    net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "\n",
    "best_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67201b-6c5d-4ed4-9fe9-4dd3aa8ad175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Train epoch 0: [0/318 (0%)]\tLoss: 22.133 |\tMSE loss: 0.028 |\tBpp loss: 4.01 |\tAux loss: 7899.10\n",
      "Train epoch 0: [160/318 (50%)]\tLoss: 31.995 |\tMSE loss: 0.043 |\tBpp loss: 4.01 |\tAux loss: 7895.50\n",
      "Test epoch 0: Average losses:\tLoss: 23.009 |\tMSE loss: 0.029 |\tBpp loss: 4.00 |\tAux loss: 7892.14\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 1: [0/318 (0%)]\tLoss: 21.483 |\tMSE loss: 0.027 |\tBpp loss: 4.00 |\tAux loss: 7891.82\n",
      "Train epoch 1: [160/318 (50%)]\tLoss: 16.069 |\tMSE loss: 0.019 |\tBpp loss: 3.99 |\tAux loss: 7887.92\n",
      "Test epoch 1: Average losses:\tLoss: 25.086 |\tMSE loss: 0.032 |\tBpp loss: 3.98 |\tAux loss: 7884.15\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 2: [0/318 (0%)]\tLoss: 27.356 |\tMSE loss: 0.036 |\tBpp loss: 3.98 |\tAux loss: 7883.79\n",
      "Train epoch 2: [160/318 (50%)]\tLoss: 17.128 |\tMSE loss: 0.020 |\tBpp loss: 3.97 |\tAux loss: 7879.62\n",
      "Test epoch 2: Average losses:\tLoss: 21.784 |\tMSE loss: 0.027 |\tBpp loss: 3.96 |\tAux loss: 7875.67\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 3: [0/318 (0%)]\tLoss: 15.361 |\tMSE loss: 0.018 |\tBpp loss: 3.96 |\tAux loss: 7875.29\n",
      "Train epoch 3: [160/318 (50%)]\tLoss: 27.206 |\tMSE loss: 0.036 |\tBpp loss: 3.95 |\tAux loss: 7870.70\n",
      "Test epoch 3: Average losses:\tLoss: 19.772 |\tMSE loss: 0.024 |\tBpp loss: 3.94 |\tAux loss: 7866.27\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 4: [0/318 (0%)]\tLoss: 20.598 |\tMSE loss: 0.026 |\tBpp loss: 3.94 |\tAux loss: 7865.81\n",
      "Train epoch 4: [160/318 (50%)]\tLoss: 19.075 |\tMSE loss: 0.023 |\tBpp loss: 3.93 |\tAux loss: 7860.82\n",
      "Test epoch 4: Average losses:\tLoss: 20.672 |\tMSE loss: 0.026 |\tBpp loss: 3.92 |\tAux loss: 7856.60\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 5: [0/318 (0%)]\tLoss: 14.836 |\tMSE loss: 0.017 |\tBpp loss: 3.92 |\tAux loss: 7856.22\n",
      "Train epoch 5: [160/318 (50%)]\tLoss: 14.803 |\tMSE loss: 0.017 |\tBpp loss: 3.91 |\tAux loss: 7851.59\n",
      "Test epoch 5: Average losses:\tLoss: 14.150 |\tMSE loss: 0.016 |\tBpp loss: 3.90 |\tAux loss: 7847.30\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 6: [0/318 (0%)]\tLoss: 12.939 |\tMSE loss: 0.014 |\tBpp loss: 3.90 |\tAux loss: 7846.88\n",
      "Train epoch 6: [160/318 (50%)]\tLoss: 12.008 |\tMSE loss: 0.012 |\tBpp loss: 3.89 |\tAux loss: 7841.52\n",
      "Test epoch 6: Average losses:\tLoss: 14.170 |\tMSE loss: 0.016 |\tBpp loss: 3.87 |\tAux loss: 7835.21\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 7: [0/318 (0%)]\tLoss: 14.660 |\tMSE loss: 0.017 |\tBpp loss: 3.87 |\tAux loss: 7834.62\n",
      "Train epoch 7: [160/318 (50%)]\tLoss: 11.094 |\tMSE loss: 0.011 |\tBpp loss: 3.86 |\tAux loss: 7828.21\n",
      "Test epoch 7: Average losses:\tLoss: 13.911 |\tMSE loss: 0.015 |\tBpp loss: 3.84 |\tAux loss: 7822.34\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 8: [0/318 (0%)]\tLoss: 18.620 |\tMSE loss: 0.023 |\tBpp loss: 3.84 |\tAux loss: 7821.88\n",
      "Train epoch 8: [160/318 (50%)]\tLoss: 9.325 |\tMSE loss: 0.008 |\tBpp loss: 3.83 |\tAux loss: 7817.16\n",
      "Test epoch 8: Average losses:\tLoss: 11.339 |\tMSE loss: 0.012 |\tBpp loss: 3.82 |\tAux loss: 7812.85\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 9: [0/318 (0%)]\tLoss: 9.739 |\tMSE loss: 0.009 |\tBpp loss: 3.82 |\tAux loss: 7812.33\n",
      "Train epoch 9: [160/318 (50%)]\tLoss: 10.590 |\tMSE loss: 0.010 |\tBpp loss: 3.81 |\tAux loss: 7806.55\n",
      "Test epoch 9: Average losses:\tLoss: 11.954 |\tMSE loss: 0.013 |\tBpp loss: 3.80 |\tAux loss: 7800.33\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 10: [0/318 (0%)]\tLoss: 12.260 |\tMSE loss: 0.013 |\tBpp loss: 3.80 |\tAux loss: 7799.60\n",
      "Train epoch 10: [160/318 (50%)]\tLoss: 9.425 |\tMSE loss: 0.009 |\tBpp loss: 3.78 |\tAux loss: 7791.42\n",
      "Test epoch 10: Average losses:\tLoss: 10.991 |\tMSE loss: 0.011 |\tBpp loss: 3.76 |\tAux loss: 7784.46\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 11: [0/318 (0%)]\tLoss: 9.765 |\tMSE loss: 0.009 |\tBpp loss: 3.77 |\tAux loss: 7783.87\n",
      "Train epoch 11: [160/318 (50%)]\tLoss: 12.464 |\tMSE loss: 0.013 |\tBpp loss: 3.75 |\tAux loss: 7777.88\n",
      "Test epoch 11: Average losses:\tLoss: 11.936 |\tMSE loss: 0.013 |\tBpp loss: 3.74 |\tAux loss: 7772.88\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 12: [0/318 (0%)]\tLoss: 9.573 |\tMSE loss: 0.009 |\tBpp loss: 3.74 |\tAux loss: 7772.48\n",
      "Train epoch 12: [160/318 (50%)]\tLoss: 12.908 |\tMSE loss: 0.014 |\tBpp loss: 3.73 |\tAux loss: 7768.14\n",
      "Test epoch 12: Average losses:\tLoss: 10.204 |\tMSE loss: 0.010 |\tBpp loss: 3.72 |\tAux loss: 7764.08\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 13: [0/318 (0%)]\tLoss: 8.462 |\tMSE loss: 0.007 |\tBpp loss: 3.73 |\tAux loss: 7763.70\n",
      "Train epoch 13: [160/318 (50%)]\tLoss: 10.004 |\tMSE loss: 0.010 |\tBpp loss: 3.72 |\tAux loss: 7759.13\n",
      "Test epoch 13: Average losses:\tLoss: 10.120 |\tMSE loss: 0.010 |\tBpp loss: 3.71 |\tAux loss: 7754.78\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 14: [0/318 (0%)]\tLoss: 11.272 |\tMSE loss: 0.012 |\tBpp loss: 3.71 |\tAux loss: 7754.34\n",
      "Train epoch 14: [160/318 (50%)]\tLoss: 10.333 |\tMSE loss: 0.010 |\tBpp loss: 3.70 |\tAux loss: 7749.29\n",
      "Test epoch 14: Average losses:\tLoss: 10.039 |\tMSE loss: 0.010 |\tBpp loss: 3.69 |\tAux loss: 7744.73\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 15: [0/318 (0%)]\tLoss: 9.836 |\tMSE loss: 0.009 |\tBpp loss: 3.69 |\tAux loss: 7744.28\n",
      "Train epoch 15: [160/318 (50%)]\tLoss: 8.087 |\tMSE loss: 0.007 |\tBpp loss: 3.68 |\tAux loss: 7739.13\n",
      "Test epoch 15: Average losses:\tLoss: 10.090 |\tMSE loss: 0.010 |\tBpp loss: 3.67 |\tAux loss: 7734.47\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 16: [0/318 (0%)]\tLoss: 10.061 |\tMSE loss: 0.010 |\tBpp loss: 3.67 |\tAux loss: 7734.03\n",
      "Train epoch 16: [160/318 (50%)]\tLoss: 10.805 |\tMSE loss: 0.011 |\tBpp loss: 3.66 |\tAux loss: 7729.23\n",
      "Test epoch 16: Average losses:\tLoss: 10.175 |\tMSE loss: 0.010 |\tBpp loss: 3.65 |\tAux loss: 7724.84\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 17: [0/318 (0%)]\tLoss: 7.406 |\tMSE loss: 0.006 |\tBpp loss: 3.65 |\tAux loss: 7724.46\n",
      "Train epoch 17: [160/318 (50%)]\tLoss: 11.883 |\tMSE loss: 0.013 |\tBpp loss: 3.64 |\tAux loss: 7719.73\n",
      "Test epoch 17: Average losses:\tLoss: 10.358 |\tMSE loss: 0.010 |\tBpp loss: 3.63 |\tAux loss: 7715.50\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 18: [0/318 (0%)]\tLoss: 12.022 |\tMSE loss: 0.013 |\tBpp loss: 3.63 |\tAux loss: 7715.11\n",
      "Train epoch 18: [160/318 (50%)]\tLoss: 11.924 |\tMSE loss: 0.013 |\tBpp loss: 3.63 |\tAux loss: 7710.33\n",
      "Test epoch 18: Average losses:\tLoss: 10.343 |\tMSE loss: 0.010 |\tBpp loss: 3.62 |\tAux loss: 7705.67\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 19: [0/318 (0%)]\tLoss: 8.808 |\tMSE loss: 0.008 |\tBpp loss: 3.62 |\tAux loss: 7705.23\n",
      "Train epoch 19: [160/318 (50%)]\tLoss: 9.662 |\tMSE loss: 0.009 |\tBpp loss: 3.61 |\tAux loss: 7700.21\n",
      "Test epoch 19: Average losses:\tLoss: 10.242 |\tMSE loss: 0.010 |\tBpp loss: 3.60 |\tAux loss: 7695.83\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 20: [0/318 (0%)]\tLoss: 6.331 |\tMSE loss: 0.004 |\tBpp loss: 3.60 |\tAux loss: 7695.43\n",
      "Train epoch 20: [160/318 (50%)]\tLoss: 11.686 |\tMSE loss: 0.012 |\tBpp loss: 3.59 |\tAux loss: 7690.54\n",
      "Test epoch 20: Average losses:\tLoss: 10.323 |\tMSE loss: 0.010 |\tBpp loss: 3.58 |\tAux loss: 7685.66\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 21: [0/318 (0%)]\tLoss: 10.385 |\tMSE loss: 0.010 |\tBpp loss: 3.58 |\tAux loss: 7685.22\n",
      "Train epoch 21: [160/318 (50%)]\tLoss: 7.318 |\tMSE loss: 0.006 |\tBpp loss: 3.57 |\tAux loss: 7680.18\n",
      "Test epoch 21: Average losses:\tLoss: 10.043 |\tMSE loss: 0.010 |\tBpp loss: 3.57 |\tAux loss: 7675.41\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 22: [0/318 (0%)]\tLoss: 14.976 |\tMSE loss: 0.018 |\tBpp loss: 3.57 |\tAux loss: 7674.98\n",
      "Train epoch 22: [160/318 (50%)]\tLoss: 8.609 |\tMSE loss: 0.008 |\tBpp loss: 3.56 |\tAux loss: 7669.81\n",
      "Test epoch 22: Average losses:\tLoss: 9.548 |\tMSE loss: 0.009 |\tBpp loss: 3.55 |\tAux loss: 7665.00\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 23: [0/318 (0%)]\tLoss: 10.074 |\tMSE loss: 0.010 |\tBpp loss: 3.55 |\tAux loss: 7664.45\n",
      "Train epoch 23: [160/318 (50%)]\tLoss: 9.144 |\tMSE loss: 0.009 |\tBpp loss: 3.54 |\tAux loss: 7658.97\n",
      "Test epoch 23: Average losses:\tLoss: 11.044 |\tMSE loss: 0.012 |\tBpp loss: 3.53 |\tAux loss: 7654.61\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 24: [0/318 (0%)]\tLoss: 13.272 |\tMSE loss: 0.015 |\tBpp loss: 3.53 |\tAux loss: 7654.19\n",
      "Train epoch 24: [160/318 (50%)]\tLoss: 12.922 |\tMSE loss: 0.014 |\tBpp loss: 3.52 |\tAux loss: 7649.66\n",
      "Test epoch 24: Average losses:\tLoss: 10.137 |\tMSE loss: 0.010 |\tBpp loss: 3.52 |\tAux loss: 7646.15\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 25: [0/318 (0%)]\tLoss: 11.831 |\tMSE loss: 0.013 |\tBpp loss: 3.52 |\tAux loss: 7645.86\n",
      "Train epoch 25: [160/318 (50%)]\tLoss: 8.910 |\tMSE loss: 0.008 |\tBpp loss: 3.51 |\tAux loss: 7642.23\n",
      "Test epoch 25: Average losses:\tLoss: 9.284 |\tMSE loss: 0.009 |\tBpp loss: 3.51 |\tAux loss: 7638.91\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 26: [0/318 (0%)]\tLoss: 6.406 |\tMSE loss: 0.004 |\tBpp loss: 3.51 |\tAux loss: 7638.65\n",
      "Train epoch 26: [160/318 (50%)]\tLoss: 9.122 |\tMSE loss: 0.009 |\tBpp loss: 3.50 |\tAux loss: 7635.08\n",
      "Test epoch 26: Average losses:\tLoss: 10.058 |\tMSE loss: 0.010 |\tBpp loss: 3.50 |\tAux loss: 7631.74\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 27: [0/318 (0%)]\tLoss: 9.801 |\tMSE loss: 0.010 |\tBpp loss: 3.49 |\tAux loss: 7631.46\n",
      "Train epoch 27: [160/318 (50%)]\tLoss: 9.941 |\tMSE loss: 0.010 |\tBpp loss: 3.49 |\tAux loss: 7627.73\n",
      "Test epoch 27: Average losses:\tLoss: 9.149 |\tMSE loss: 0.009 |\tBpp loss: 3.48 |\tAux loss: 7624.18\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 28: [0/318 (0%)]\tLoss: 8.895 |\tMSE loss: 0.008 |\tBpp loss: 3.48 |\tAux loss: 7623.88\n",
      "Train epoch 28: [160/318 (50%)]\tLoss: 9.319 |\tMSE loss: 0.009 |\tBpp loss: 3.48 |\tAux loss: 7620.05\n",
      "Test epoch 28: Average losses:\tLoss: 9.039 |\tMSE loss: 0.009 |\tBpp loss: 3.47 |\tAux loss: 7616.42\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 29: [0/318 (0%)]\tLoss: 7.137 |\tMSE loss: 0.006 |\tBpp loss: 3.47 |\tAux loss: 7616.11\n",
      "Train epoch 29: [160/318 (50%)]\tLoss: 7.551 |\tMSE loss: 0.006 |\tBpp loss: 3.47 |\tAux loss: 7612.14\n",
      "Test epoch 29: Average losses:\tLoss: 9.439 |\tMSE loss: 0.009 |\tBpp loss: 3.46 |\tAux loss: 7608.45\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 30: [0/318 (0%)]\tLoss: 10.505 |\tMSE loss: 0.011 |\tBpp loss: 3.46 |\tAux loss: 7608.14\n",
      "Train epoch 30: [160/318 (50%)]\tLoss: 10.185 |\tMSE loss: 0.010 |\tBpp loss: 3.45 |\tAux loss: 7603.97\n",
      "Test epoch 30: Average losses:\tLoss: 8.519 |\tMSE loss: 0.008 |\tBpp loss: 3.45 |\tAux loss: 7599.91\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 31: [0/318 (0%)]\tLoss: 10.701 |\tMSE loss: 0.011 |\tBpp loss: 3.45 |\tAux loss: 7599.50\n",
      "Train epoch 31: [160/318 (50%)]\tLoss: 7.158 |\tMSE loss: 0.006 |\tBpp loss: 3.44 |\tAux loss: 7594.58\n",
      "Test epoch 31: Average losses:\tLoss: 8.320 |\tMSE loss: 0.008 |\tBpp loss: 3.43 |\tAux loss: 7590.27\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 32: [0/318 (0%)]\tLoss: 7.903 |\tMSE loss: 0.007 |\tBpp loss: 3.43 |\tAux loss: 7589.81\n",
      "Train epoch 32: [160/318 (50%)]\tLoss: 8.284 |\tMSE loss: 0.007 |\tBpp loss: 3.42 |\tAux loss: 7583.16\n",
      "Test epoch 32: Average losses:\tLoss: 7.646 |\tMSE loss: 0.007 |\tBpp loss: 3.41 |\tAux loss: 7576.77\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 33: [0/318 (0%)]\tLoss: 8.295 |\tMSE loss: 0.008 |\tBpp loss: 3.41 |\tAux loss: 7576.22\n",
      "Train epoch 33: [160/318 (50%)]\tLoss: 8.519 |\tMSE loss: 0.008 |\tBpp loss: 3.40 |\tAux loss: 7568.44\n",
      "Test epoch 33: Average losses:\tLoss: 8.039 |\tMSE loss: 0.007 |\tBpp loss: 3.38 |\tAux loss: 7560.98\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 34: [0/318 (0%)]\tLoss: 7.805 |\tMSE loss: 0.007 |\tBpp loss: 3.38 |\tAux loss: 7560.31\n",
      "Train epoch 34: [160/318 (50%)]\tLoss: 10.351 |\tMSE loss: 0.011 |\tBpp loss: 3.37 |\tAux loss: 7552.51\n",
      "Test epoch 34: Average losses:\tLoss: 8.884 |\tMSE loss: 0.009 |\tBpp loss: 3.35 |\tAux loss: 7543.81\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 35: [0/318 (0%)]\tLoss: 7.829 |\tMSE loss: 0.007 |\tBpp loss: 3.35 |\tAux loss: 7543.01\n",
      "Train epoch 35: [160/318 (50%)]\tLoss: 8.940 |\tMSE loss: 0.009 |\tBpp loss: 3.34 |\tAux loss: 7535.22\n",
      "Test epoch 35: Average losses:\tLoss: 7.650 |\tMSE loss: 0.007 |\tBpp loss: 3.33 |\tAux loss: 7529.10\n",
      "\n",
      "Learning rate: 0.0001\n",
      "Train epoch 36: [0/318 (0%)]\tLoss: 10.802 |\tMSE loss: 0.011 |\tBpp loss: 3.33 |\tAux loss: 7528.58\n",
      "Train epoch 36: [160/318 (50%)]\tLoss: 6.450 |\tMSE loss: 0.005 |\tBpp loss: 3.32 |\tAux loss: 7522.76\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(last_epoch, epochs):\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    train_one_epoch(\n",
    "        net,\n",
    "        criterion,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        aux_optimizer,\n",
    "        epoch,\n",
    "        clip_max_norm,\n",
    "    )\n",
    "    loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
    "    lr_scheduler.step(loss)\n",
    "\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    if save:\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": net.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            },\n",
    "            is_best,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
