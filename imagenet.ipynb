{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ad3d1f-2839-4a43-9972-989664a720bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870f4f1a-79c5-4196-9655-30e213f4b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_decoder(compressed_img,original_shape):\n",
    "    decompressed = zlib.decompress(compressed_img)\n",
    "    ẑ = np.frombuffer(decompressed, dtype=np.int8)\n",
    "    ẑ = ẑ.reshape(original_shape)\n",
    "    return ẑ\n",
    "def batch_entropy_decode(compressed_batch,original_size):\n",
    "    z = torch.cat([\n",
    "        torch.tensor(\n",
    "            entropy_decoder(compressed_batch[i], original_size)\n",
    "        ) \n",
    "        for i in range(batch_size)\n",
    "    ])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f44b29-283d-40d8-9a26-df132b5e149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"danjacobellis/imagenet_RDAE_batched_250k\",split='train').with_format(\"torch\")\n",
    "N_classes=1001;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "150e9f34-4fa4-47bb-a6b2-7477603b7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 34s, sys: 15.8 s, total: 10min 50s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "for (i_batch, batch) in enumerate(dataset):\n",
    "    latent_size = batch['latent_size']\n",
    "    batch_size = latent_size[0];\n",
    "    original_size = latent_size.numpy().copy();\n",
    "    original_size[0] = 1;\n",
    "    \n",
    "    compressed_batch = batch['compressed_batch']\n",
    "    z = batch_entropy_decode(compressed_batch,original_size);\n",
    "    z.to(\"cuda\")\n",
    "    \n",
    "    label = batch['label'].to(torch.int64)\n",
    "    y = torch.zeros((batch_size,N_classes,1,1),dtype=torch.int16)\n",
    "    y.scatter_(1, label.view(-1, 1, 1, 1), 1)\n",
    "    y.to(\"cuda\")\n",
    "    \n",
    "    z.detach()\n",
    "    y.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5f23ab6-7e03-4855-a58a-978a24691c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 45, 63])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6c9c5c1-38b7-40c1-b7a4-a14f8b423a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1001, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e75526-23a5-4429-9f68-e5016fa229a3",
   "metadata": {},
   "source": [
    "GPT-4 prompt:\n",
    "\n",
    "The input `z` is a [64, 128, width, height] tensor containing image a batch of 64 (128, width, height) image features. The images are variable sized but each batch contains images of the same size.\n",
    "\n",
    "The label `y` is a batch of 64 one-hot encoded imagenet labels and has size [64, 1001, 1, 1]\n",
    "\n",
    "The desired architecture is a fully convolutional network (FCN)\n",
    "```\n",
    "64 x  128   x ~128 x ~128 <- input is batches of 64 by 128 by width by height\n",
    "64 x  256   x ~64  x ~64\n",
    "64 x  512   x ~32  x ~32\n",
    "64 x  1024  x ~16  x ~16\n",
    "64 x  2048  x ~8   x ~8\n",
    "64 x  4096  x ~4   x ~4\n",
    "64 x  8192  x ~2   x ~2\n",
    "64 x 16384  x  1   x  1\n",
    "64 x  1000  x  1   x  1\n",
    "```\n",
    "\n",
    "Write a class Network that inherits from nn.module that implements this network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1b4b5-8674-4f72-9e35-159a683d1827",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "To create a Fully Convolutional Network (FCN) that follows the outlined architecture, you can utilize PyTorch's nn.Module as a base class. The FCN will consist of several convolutional layers (nn.Conv2d) with appropriate kernel sizes and strides to reduce the spatial dimensions as needed.\r\n",
    "\r\n",
    "Here's how you can define your networ\n",
    "\n",
    "```\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "class Network(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Network, self).__init__()\r\n",
    "        \r\n",
    "        self.conv1 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv3 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv4 = nn.Conv2d(1024, 2048, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv5 = nn.Conv2d(2048, 4096, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv6 = nn.Conv2d(4096, 8192, kernel_size=3, stride=2, padding=1)\r\n",
    "        self.conv7 = nn.Conv2d(8192, 16384, kernel_size=2, stride=2, padding=0)  # kernel size 2 and padding 0 to reach 1x1 size\r\n",
    "        self.conv8 = nn.Conv2d(16384, 1000, kernel_size=1)  # kernel size 1 to maintain 1x1 size\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.conv1(x))\r\n",
    "        x = F.relu(self.conv2(x))\r\n",
    "        x = F.relu(self.conv3(x))\r\n",
    "        x = F.relu(self.conv4(x))\r\n",
    "        x = F.relu(self.conv5(x))\r\n",
    "        x = F.relu(self.conv6(x))\r\n",
    "        x = F.relu(self.conv7(x))\r\n",
    "        x = self.conv8(x)  # no activation on the las\n",
    "```\n",
    "\n",
    "In the __init__ method, we define the architecture of the network. Each convolutional layer reduces the spatial dimensions of the input roughly by half, due to the stride of 2.\r\n",
    "\r\n",
    "In the forward method, we apply each layer to the input x sequentially, with a ReLU activation function after each layer except for the last one. For the last layer, we don't use an activation function because this is a multi-class classification problem, and we'll likely use a loss function such as CrossEntropyLoss that includes a softmax activation.\r\n",
    "\r\n",
    "Please note that the actual reduction in dimensions may vary depending on the exact sizes of your input images, due to the rounding behavior of the strided convolutions. Also, you'll need to ensure that your input images are large enough to support this many downsampling steps.\r\n",
    "\r\n",
    "Regarding the loading and preprocessing of the data, your current code seems to be doing that correctly. However, you might want to consider moving the data loading and preprocessing code into a PyTorch DataLoader or similar, which can handle batching and shuffling of the data, and potentially speed up training by loading data in parallel.t layer\r\n",
    "        return xk:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
