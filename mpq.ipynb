{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5589cc93-57ed-4f9d-90c8-95ea6b02fc98",
   "metadata": {},
   "source": [
    "Machine perceptual quality evaluation\n",
    "\n",
    "* Images\n",
    "  * Dataset: [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)\n",
    "  * Model: [Distilled data-efficient Image Transformer (DeiT)](https://huggingface.co/facebook/deit-small-distilled-patch16-224)\n",
    "  * Metric: Image classification accuracy\n",
    "  * Compression:\n",
    "    * JPEG Q=5/100\n",
    "    * HIFIC\n",
    "    * TFCI\n",
    "* Audio\n",
    "  * Dataset: [Common Voice Corpus 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)\n",
    "  * Model: [Whisper](https://huggingface.co/openai/whisper-small)\n",
    "  * Metric: Speech recognition word error rate\n",
    "  * Compression:\n",
    "    * MP3 kbps\n",
    "    * Descript\n",
    "    * Encodec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bb93100c-8646-47f3-bc77-0dd6ef36981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import PIL\n",
    "import torchaudio\n",
    "from evaluate import evaluator\n",
    "from transformers import pipeline\n",
    "from io import BytesIO\n",
    "import encodec\n",
    "import torch\n",
    "\n",
    "from compressai.zoo import bmshj2018_factorized\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b8a459-6e2b-4704-81b7-260e99817367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_compress_quality_5(sample):\n",
    "    img = sample['image']\n",
    "    with BytesIO() as f:\n",
    "        img.save(f, format='JPEG',quality=5)\n",
    "        img = f.getvalue()\n",
    "    sample['image'] = img\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ebdbb014-f25c-4a23-b769-65174d8b67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_compress_quality_8(sample):\n",
    "    try:\n",
    "        audio = sample['audio']['array'].unsqueeze(0)\n",
    "        fs = sample['audio']['sampling_rate']\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, audio, sample_rate=fs, format=\"mp3\", compression=8)\n",
    "            f.seek(0)\n",
    "            audio = torchaudio.load(f,format=\"mp3\")\n",
    "        sample['audio']['array'] = audio[0][0]\n",
    "    except:\n",
    "        pass\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26cb9b-91e1-4e60-ad77-3546ce3c65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encodec.EncodecModel.encodec_model_48khz()\n",
    "model.set_target_bandwidth(3)\n",
    "\n",
    "def encodec_compress(sample):\n",
    "    audio = sample['audio']['array'].unsqueeze(0)\n",
    "    fs = sample['audio']['sampling_rate']\n",
    "    audio = encodec.utils.convert_audio(audio,fs,model.sample_rate,model.channels)\n",
    "    with torch.no_grad():\n",
    "        encoded_frames = model.encode(audio.unsqueeze(0))\n",
    "        sample['audio']['array'] = model.decode(encoded_frames).mean(dim=[0,1]) \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "100af2aa-ccf9-4103-bb93-fdf12fd4698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"imagenet-1k\", split=\"validation[:100]\")\n",
    "jpeg_q5 = data.map(jpeg_compress_quality_5)\n",
    "rdae = data.map(rdae_compress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0230ebe-b2f7-4f4a-8e1a-68b3a2fe6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = bmshj2018_factorized(quality=2, pretrained=True).eval()\n",
    "def rdae_compress(sample):\n",
    "    img = sample['image']\n",
    "\n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out_net = net.forward(x)\n",
    "    out_net['x_hat'].clamp_(0, 1)\n",
    "    sample['image'] = transforms.ToPILImage()(out_net['x_hat'].squeeze())\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "34f83e94-0458-41dc-a8ac-69fc245de73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"facebook/deit-small-distilled-patch16-224\"\n",
    ")\n",
    "\n",
    "task_evaluator = evaluator(\"image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33b1c81-2c1d-4ab6-ac8e-e15a2fa8eb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.81,\n",
       " 'total_time_in_seconds': 7.377623004023917,\n",
       " 'samples_per_second': 13.554501218815032,\n",
       " 'latency_in_seconds': 0.07377623004023917}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e81a34a-cda3-42ff-8e31-4a04d75291af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7,\n",
       " 'total_time_in_seconds': 7.054108065029141,\n",
       " 'samples_per_second': 14.1761366679016,\n",
       " 'latency_in_seconds': 0.0705410806502914}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=jpeg_q5,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b3436a39-e3fc-423e-9792-0d8053957c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.78,\n",
       " 'total_time_in_seconds': 8.429108447977342,\n",
       " 'samples_per_second': 11.863650897028869,\n",
       " 'latency_in_seconds': 0.08429108447977342}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=rdae,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f458507a-60b7-425a-a1ac-94415b311de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"validation[:40]\")\n",
    "mp3_q8 = data.with_format(\"torch\").map(mp3_compress_quality_8).with_format('numpy')\n",
    "encodec_q48 = data.with_format(\"torch\").map(encodec_compress).with_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb09bcb8-65d7-44b0-aade-7e9b1e23a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    ")\n",
    "\n",
    "task_evaluator = evaluator(\"automatic-speech-recognition\")\n",
    "task_evaluator.PIPELINE_KWARGS.pop('truncation', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e2f09aee-1e93-4e6f-a372-f23fc9b27261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1358: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wer': 0.24324324324324326,\n",
       " 'total_time_in_seconds': 129.5941981250071,\n",
       " 'samples_per_second': 0.3086557930735127,\n",
       " 'latency_in_seconds': 3.2398549531251777}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3fe820a0-c277-4022-bdae-ad41ce3f06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.327027027027027,\n",
       " 'total_time_in_seconds': 130.1211449749535,\n",
       " 'samples_per_second': 0.3074058409776477,\n",
       " 'latency_in_seconds': 3.2530286243738376}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=mp3_q8,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2590f35e-6f54-433a-bfb4-70454dd2f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1358: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wer': 0.5054054054054054,\n",
       " 'total_time_in_seconds': 135.95577442500507,\n",
       " 'samples_per_second': 0.2942133217156179,\n",
       " 'latency_in_seconds': 3.398894360625127}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=encodec_q48,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
