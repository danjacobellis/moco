{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5589cc93-57ed-4f9d-90c8-95ea6b02fc98",
   "metadata": {},
   "source": [
    "Machine perceptual quality evaluation\n",
    "\n",
    "* Images\n",
    "  * Dataset: [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)\n",
    "  * Model: [Distilled data-efficient Image Transformer (DeiT)](https://huggingface.co/facebook/deit-small-distilled-patch16-224)\n",
    "  * Metric: Image classification accuracy\n",
    "  * Compression:\n",
    "    * JPEG Q=5/100\n",
    "    * HIFIC\n",
    "    * TFCI\n",
    "* Audio\n",
    "  * Dataset: [Common Voice Corpus 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)\n",
    "  * Model: [Whisper](https://huggingface.co/openai/whisper-small)\n",
    "  * Metric: Speech recognition word error rate\n",
    "  * Compression:\n",
    "    * MP3 kbps\n",
    "    * Descript\n",
    "    * Encodec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb93100c-8646-47f3-bc77-0dd6ef36981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import PIL\n",
    "import torchaudio\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from transformers import pipeline\n",
    "from io import BytesIO\n",
    "import encodec\n",
    "import torch\n",
    "import numpy as np\n",
    "from compressai.zoo import bmshj2018_factorized\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b8a459-6e2b-4704-81b7-260e99817367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jpeg_compress_quality_5(sample):\n",
    "    img = sample['image']\n",
    "    with BytesIO() as f:\n",
    "        img.save(f, format='JPEG',quality=5)\n",
    "        img = f.getvalue()\n",
    "    sample['image'] = img\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0230ebe-b2f7-4f4a-8e1a-68b3a2fe6ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://compressai.s3.amazonaws.com/models/v1/bmshj2018-factorized-prior-2-87279a02.pth.tar\" to /home/dan/.cache/torch/hub/checkpoints/bmshj2018-factorized-prior-2-87279a02.pth.tar\n",
      "100%|██████████████████████████| 11.5M/11.5M [00:02<00:00, 5.51MB/s]\n"
     ]
    }
   ],
   "source": [
    "net = bmshj2018_factorized(quality=2, pretrained=True).eval()\n",
    "def rdae_compress(sample):\n",
    "    img = sample['image']\n",
    "\n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out_net = net.forward(x)\n",
    "    out_net['x_hat'].clamp_(0, 1)\n",
    "    sample['image'] = transforms.ToPILImage()(out_net['x_hat'].squeeze())\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdbb014-f25c-4a23-b769-65174d8b67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_compress_quality_8(sample):\n",
    "    try:\n",
    "        audio = sample['audio']['array'].unsqueeze(0)\n",
    "        fs = sample['audio']['sampling_rate']\n",
    "        with BytesIO() as f:\n",
    "            torchaudio.save(f, audio, sample_rate=fs, format=\"mp3\", compression=8)\n",
    "            f.seek(0)\n",
    "            audio = torchaudio.load(f,format=\"mp3\")\n",
    "        sample['audio']['array'] = audio[0][0]\n",
    "    except:\n",
    "        pass\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea26cb9b-91e1-4e60-ad77-3546ce3c65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encodec.EncodecModel.encodec_model_48khz()\n",
    "model.set_target_bandwidth(3)\n",
    "\n",
    "def encodec_compress(sample):\n",
    "    audio = sample['audio']['array'].unsqueeze(0)\n",
    "    fs = sample['audio']['sampling_rate']\n",
    "    audio = encodec.utils.convert_audio(audio,fs,model.sample_rate,model.channels)\n",
    "    with torch.no_grad():\n",
    "        encoded_frames = model.encode(audio.unsqueeze(0))\n",
    "        sample['audio']['array'] = model.decode(encoded_frames).mean(dim=[0,1]) \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100af2aa-ccf9-4103-bb93-fdf12fd4698c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3479bbfe7bfb446dbe23a8a9cb458165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"imagenet-1k\", split=\"validation[:100]\")\n",
    "jpeg_q5 = data.map(jpeg_compress_quality_5)\n",
    "rdae = data.map(rdae_compress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f83e94-0458-41dc-a8ac-69fc245de73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"facebook/deit-small-distilled-patch16-224\"\n",
    ")\n",
    "\n",
    "task_evaluator = evaluator(\"image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b33b1c81-2c1d-4ab6-ac8e-e15a2fa8eb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.81,\n",
       " 'total_time_in_seconds': 2.0621120198629797,\n",
       " 'samples_per_second': 48.49397076238596,\n",
       " 'latency_in_seconds': 0.020621120198629796}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e81a34a-cda3-42ff-8e31-4a04d75291af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7,\n",
       " 'total_time_in_seconds': 1.8150526769459248,\n",
       " 'samples_per_second': 55.09481970973081,\n",
       " 'latency_in_seconds': 0.018150526769459246}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=jpeg_q5,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3436a39-e3fc-423e-9792-0d8053957c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.78,\n",
       " 'total_time_in_seconds': 2.293169038835913,\n",
       " 'samples_per_second': 43.60777522566031,\n",
       " 'latency_in_seconds': 0.02293169038835913}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=rdae,\n",
    "    metric=\"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f458507a-60b7-425a-a1ac-94415b311de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9437479d691405f8e4072e6cec79349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/.local/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:416: UserWarning: File-like object support in sox_io backend is deprecated, and will be removed in v2.1. See https://github.com/pytorch/audio/issues/2950 for the detail.Please migrate to the new dispatcher, or use soundfile backend.\n",
      "  warnings.warn(_deprecation_message)\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n",
      "formats: mp3 can't encode MPEG audio (layer I, II or III) to 16-bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b19cab92a3c4749bae5d9c5189d8169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"validation[:40]\")\n",
    "mp3_q8 = data.with_format(\"torch\").map(mp3_compress_quality_8).with_format('numpy')\n",
    "encodec_q48 = data.with_format(\"torch\").map(encodec_compress).with_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb09bcb8-65d7-44b0-aade-7e9b1e23a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    ")\n",
    "\n",
    "task_evaluator = evaluator(\"automatic-speech-recognition\")\n",
    "task_evaluator.PIPELINE_KWARGS.pop('truncation', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f09aee-1e93-4e6f-a372-f23fc9b27261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.24324324324324326,\n",
       " 'total_time_in_seconds': 31.928794587031007,\n",
       " 'samples_per_second': 1.2527876644690932,\n",
       " 'latency_in_seconds': 0.7982198646757752}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe820a0-c277-4022-bdae-ad41ce3f06b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.327027027027027,\n",
       " 'total_time_in_seconds': 34.731284122914076,\n",
       " 'samples_per_second': 1.1516994263281464,\n",
       " 'latency_in_seconds': 0.8682821030728519}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=mp3_q8,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2590f35e-6f54-433a-bfb4-70454dd2f360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.5054054054054054,\n",
       " 'total_time_in_seconds': 34.0187990241684,\n",
       " 'samples_per_second': 1.1758204624326185,\n",
       " 'latency_in_seconds': 0.85046997560421}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=encodec_q48,\n",
    "    input_column=\"audio\",\n",
    "    label_column=\"sentence\",\n",
    "    metric=\"wer\",\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19bafed3-e3fd-4e1c-ab5f-d66d79f17c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"scene_parse_150\",split='validation[:100]')\n",
    "mean_iou = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c21045f1-a6e3-4a45-9fd5-03788386bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3936678e-484f-4055-b137-111e4bc60f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 4.18 s, total: 2min 51s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = []\n",
    "y = []\n",
    "for sample in data:\n",
    "    ground_truth = sample['annotation']\n",
    "    ground_truth = ground_truth.resize((128,128))\n",
    "    img = sample['image']\n",
    "    img = img.resize((512,512))\n",
    "    inputs = extractor(images=img, return_tensors=\"pt\",do_resize=False)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "    predictions = logits.argmax(dim=1)[0].detach()\n",
    "\n",
    "    x.append(np.array(ground_truth))\n",
    "    y.append(np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "478645a7-5731-4ea0-bebb-2be41d040516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.93 s, sys: 57.2 ms, total: 5.99 s\n",
      "Wall time: 6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21408163470611238"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mean_iou.compute(\n",
    "    predictions=y,\n",
    "    references=x,\n",
    "    num_labels=150,\n",
    "    ignore_index=255,\n",
    "    reduce_labels=True\n",
    ")['mean_iou']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
