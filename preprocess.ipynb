{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e87b32-ce3d-4a7c-ab96-5f6fc6bf7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import zlib\n",
    "import numpy as np\n",
    "from compressai.entropy_models import EntropyBottleneck\n",
    "from compressai.layers import GDN\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.models.utils import conv, deconv\n",
    "from datasets import load_dataset, Dataset, Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5fb93e-3612-4828-98a5-6878d662845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(CompressionModel):\n",
    "    def __init__(self, N=128):\n",
    "        super().__init__()\n",
    "        self.entropy_bottleneck = EntropyBottleneck(N)\n",
    "        self.encode = nn.Sequential(\n",
    "            conv(3, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "        )\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.encode(x)\n",
    "        y_hat, y_likelihoods = self.entropy_bottleneck(y)\n",
    "        x_hat = self.decode(y_hat)\n",
    "        return x_hat, y_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad182bc-4c0e-4549-8492-4a00d9e9e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossy_analysis_transform(img):\n",
    "    x = img.to(\"cuda\")\n",
    "    z = net.encode(x).round().to(torch.int8).detach().to(\"cpu\").numpy()\n",
    "    return z\n",
    "    \n",
    "def lossless_entropy_encode(z):\n",
    "    original_shape = z.shape\n",
    "    compressed_img = zlib.compress(z.tobytes(), level=9)\n",
    "    return compressed_img, original_shape\n",
    "\n",
    "def prep_dataset(sample):\n",
    "    img = sample['image']\n",
    "    sample['width'] = img.width\n",
    "    sample['height'] = img.height\n",
    "\n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    \n",
    "    sample['image'] = Image().encode_example(img)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb320a9-c832-48e0-be78-3e5210d16788",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net = net.to(\"cuda\")\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39edccf4-53bb-467e-b192-86c523a6086e",
   "metadata": {},
   "source": [
    "# Training split\n",
    "\n",
    "Cutoff for training split is 2048 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbec15-c50c-4ca8-b70f-52241190c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='train')\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a258b3-1851-4d98-a2f8-ce90318f71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e5282-a6e1-4a2a-8883-3d10e6cfd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=2048):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5e715-3767-4c25-a7ca-c95104c2dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        train_dataset = train_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e299b2e-16b6-43cc-8c95-44242a85b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.push_to_hub(\"danjacobellis/imagenet_batched_64\",split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71331e1-9be7-4087-9eb8-2a071ac92a86",
   "metadata": {},
   "source": [
    "# Test split\n",
    "\n",
    "Cutoff for the test split is 256 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b8cc09-88d7-4cc9-a9d7-032081b7e2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9846889102404e92ae421c0b9dffe4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 8.55 s, total: 3min 27s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='test')\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e6ca81-5c1b-4a63-8234-3858fc33ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f8d0ad-0df3-4b05-8e6b-36151333f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=256):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd380a32-544e-4ebf-bad5-555456c2b303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ad56782e2d412188eab34d5877c9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e2f0dc5f6b49eaa38d825e100b33e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        test_dataset = test_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea0292-6671-4f16-8730-4e9e8ed9c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.push_to_hub(\"danjacobellis/imagenet_batched_64\",split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82979397-f57e-415f-a924-8ac70d74bf2f",
   "metadata": {},
   "source": [
    "# Validation split\n",
    "\n",
    "Cutoff for training split is 64 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855a36c-d113-4903-bcb6-47654964ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='validation')\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f6c15-69f4-4cb6-a753-d1bd49ba59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4a2ff-f0a8-470e-9b5e-f3e25f2df4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=64):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a90050-8ac5-457f-aa3c-a1af6a689429",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        val_dataset = val_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edace2c8-1869-40f2-a347-9c86ee27a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.push_to_hub(\"danjacobellis/imagenet_batched_64\",split='val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
