{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e87b32-ce3d-4a7c-ab96-5f6fc6bf7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad182bc-4c0e-4549-8492-4a00d9e9e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(sample):\n",
    "    img = sample['image']\n",
    "    sample['width'] = img.width\n",
    "    sample['height'] = img.height\n",
    "\n",
    "    if (img.mode == 'L') | (img.mode == 'CMYK') | (img.mode == 'RGBA'):\n",
    "        rgbimg = PIL.Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    \n",
    "    sample['image'] = Image().encode_example(img)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb320a9-c832-48e0-be78-3e5210d16788",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39edccf4-53bb-467e-b192-86c523a6086e",
   "metadata": {},
   "source": [
    "# Training split\n",
    "\n",
    "Cutoff for training split is 2048 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbec15-c50c-4ca8-b70f-52241190c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='train)\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a258b3-1851-4d98-a2f8-ce90318f71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e5282-a6e1-4a2a-8883-3d10e6cfd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=2048):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5e715-3767-4c25-a7ca-c95104c2dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        train_dataset = train_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71331e1-9be7-4087-9eb8-2a071ac92a86",
   "metadata": {},
   "source": [
    "# Test split\n",
    "\n",
    "Cutoff for the test split is 256 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8cc09-88d7-4cc9-a9d7-032081b7e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='test')\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6ca81-5c1b-4a63-8234-3858fc33ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8d0ad-0df3-4b05-8e6b-36151333f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=256):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd380a32-544e-4ebf-bad5-555456c2b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        test_dataset = test_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82979397-f57e-415f-a924-8ac70d74bf2f",
   "metadata": {},
   "source": [
    "# Validation split\n",
    "\n",
    "Cutoff for training split is 128 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855a36c-d113-4903-bcb6-47654964ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"imagenet-1k\",split='validation')\n",
    "dataset = dataset.map(prep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f6c15-69f4-4cb6-a753-d1bd49ba59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset.from_dict({\n",
    "    \"img_batch\" : [],\n",
    "    \"label_batch\" : [],\n",
    "    \"width\" : [],\n",
    "    \"height\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4a2ff-f0a8-470e-9b5e-f3e25f2df4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = torch.tensor(dataset['width']);\n",
    "height = torch.tensor(dataset['height']);\n",
    "unique_pairs = torch.unique(torch.stack([width, height], dim=1), dim=0)\n",
    "pair_counts = {(w.item(), h.item()): ((width == w) & (height == h)).sum().item() \n",
    "               for w, h in unique_pairs}\n",
    "sizes = sorted(pair_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "N = 0;\n",
    "while (sizes[N][1]>=128):\n",
    "    N +=1\n",
    "sizes = sizes[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a90050-8ac5-457f-aa3c-a1af6a689429",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for size, count in sizes:\n",
    "    w = size[0]; h = size[1]\n",
    "    filtered = dataset.filter(lambda x: x['width']==w and x['height']==h)\n",
    "    for i_batch in range(len(filtered)//batch_size):\n",
    "        ind = range(i_batch * batch_size, (i_batch + 1) * batch_size)\n",
    "        img_batch = filtered[ind]['image']\n",
    "        img_batch = [Image().encode_example(pil_img) for pil_img in img_batch] \n",
    "        label_batch = filtered[ind]['label']\n",
    "\n",
    "        val_dataset = val_dataset.add_item({\n",
    "            \"img_batch\" : img_batch,\n",
    "            \"label_batch\" : label_batch,\n",
    "            \"width\" : w,\n",
    "            \"height\": h,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425c44a-c1df-4c90-b144-861e6ef14a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e5035-8560-4129-aad4-32b219844ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"danjacobellis/imagenet_batched_64\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
